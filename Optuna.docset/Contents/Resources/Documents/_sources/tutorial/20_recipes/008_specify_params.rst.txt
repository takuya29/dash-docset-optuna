
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "tutorial/20_recipes/008_specify_params.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_tutorial_20_recipes_008_specify_params.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_tutorial_20_recipes_008_specify_params.py:


.. _specify_params:

Specify Hyperparameters Manually
================================

It's natural that you have some specific sets of hyperparameters to try first such as initial learning rate
values and the number of leaves.
Also, it's possible that you've already tried those sets before having Optuna find better
sets of hyperparameters.

Optuna provides two APIs to support such cases:

1. Passing those sets of hyperparameters and let Optuna evaluate them - :func:`~optuna.study.Study.enqueue_trial`
2. Adding the results of those sets as completed ``Trial``\s - :func:`~optuna.study.Study.add_trial`

.. _enqueue_trial_tutorial:

---------------------------------------------------------
First Scenario: Have Optuna evaluate your hyperparameters
---------------------------------------------------------

In this scenario, let's assume you have some out-of-box sets of hyperparameters but have not
evaluated them yet and decided to use Optuna to find better sets of hyperparameters.

Optuna has :func:`optuna.study.Study.enqueue_trial` which lets you pass those sets of
hyperparameters to Optuna and Optuna will evaluate them.

This section walks you through how to use this lit API with `LightGBM <https://lightgbm.readthedocs.io/en/latest/>`_.

.. GENERATED FROM PYTHON SOURCE LINES 31-41

.. code-block:: default


    import lightgbm as lgb
    import numpy as np
    import sklearn.datasets
    import sklearn.metrics
    from sklearn.model_selection import train_test_split

    import optuna









.. GENERATED FROM PYTHON SOURCE LINES 42-43

Define the objective function.

.. GENERATED FROM PYTHON SOURCE LINES 43-69

.. code-block:: default

    def objective(trial):
        data, target = sklearn.datasets.load_breast_cancer(return_X_y=True)
        train_x, valid_x, train_y, valid_y = train_test_split(data, target, test_size=0.25)
        dtrain = lgb.Dataset(train_x, label=train_y)
        dvalid = lgb.Dataset(valid_x, label=valid_y)

        param = {
            "objective": "binary",
            "metric": "auc",
            "verbosity": -1,
            "boosting_type": "gbdt",
            "bagging_fraction": min(trial.suggest_float("bagging_fraction", 0.4, 1.0 + 1e-12), 1),
            "bagging_freq": trial.suggest_int("bagging_freq", 0, 7),
            "min_child_samples": trial.suggest_int("min_child_samples", 5, 100),
        }

        # Add a callback for pruning.
        pruning_callback = optuna.integration.LightGBMPruningCallback(trial, "auc")
        gbm = lgb.train(param, dtrain, valid_sets=[dvalid], callbacks=[pruning_callback])

        preds = gbm.predict(valid_x)
        pred_labels = np.rint(preds)
        accuracy = sklearn.metrics.accuracy_score(valid_y, pred_labels)
        return accuracy









.. GENERATED FROM PYTHON SOURCE LINES 70-71

Then, construct ``Study`` for hyperparameter optimization.

.. GENERATED FROM PYTHON SOURCE LINES 71-74

.. code-block:: default


    study = optuna.create_study(direction="maximize", pruner=optuna.pruners.MedianPruner())








.. GENERATED FROM PYTHON SOURCE LINES 75-77

Here, we get Optuna evaluate some sets with larger ``"bagging_fraq"`` value and
the default values.

.. GENERATED FROM PYTHON SOURCE LINES 77-101

.. code-block:: default


    study.enqueue_trial(
        {
            "bagging_fraction": 1.0,
            "bagging_freq": 0,
            "min_child_samples": 20,
        }
    )

    study.enqueue_trial(
        {
            "bagging_fraction": 0.75,
            "bagging_freq": 5,
            "min_child_samples": 20,
        }
    )

    import logging
    import sys

    # Add stream handler of stdout to show the messages to see Optuna works expectedly.
    optuna.logging.get_logger("optuna").addHandler(logging.StreamHandler(sys.stdout))
    study.optimize(objective, n_trials=100, timeout=600)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Trial 0 finished with value: 0.972027972027972 and parameters: {'bagging_fraction': 1.0, 'bagging_freq': 0, 'min_child_samples': 20}. Best is trial 0 with value: 0.972027972027972.
    Trial 1 finished with value: 0.986013986013986 and parameters: {'bagging_fraction': 0.75, 'bagging_freq': 5, 'min_child_samples': 20}. Best is trial 1 with value: 0.986013986013986.
    Trial 2 finished with value: 0.9440559440559441 and parameters: {'bagging_fraction': 0.5506766301477889, 'bagging_freq': 7, 'min_child_samples': 78}. Best is trial 1 with value: 0.986013986013986.
    Trial 3 finished with value: 0.965034965034965 and parameters: {'bagging_fraction': 0.49214746028116485, 'bagging_freq': 5, 'min_child_samples': 70}. Best is trial 1 with value: 0.986013986013986.
    Trial 4 finished with value: 0.951048951048951 and parameters: {'bagging_fraction': 0.7494305299365314, 'bagging_freq': 1, 'min_child_samples': 100}. Best is trial 1 with value: 0.986013986013986.
    Trial 5 finished with value: 0.972027972027972 and parameters: {'bagging_fraction': 0.6463976390903002, 'bagging_freq': 3, 'min_child_samples': 71}. Best is trial 1 with value: 0.986013986013986.
    Trial 6 pruned. Trial was pruned at iteration 0.
    Trial 7 pruned. Trial was pruned at iteration 0.
    Trial 8 finished with value: 0.965034965034965 and parameters: {'bagging_fraction': 0.8378692289828823, 'bagging_freq': 2, 'min_child_samples': 42}. Best is trial 1 with value: 0.986013986013986.
    Trial 9 pruned. Trial was pruned at iteration 1.
    Trial 10 pruned. Trial was pruned at iteration 1.
    Trial 11 pruned. Trial was pruned at iteration 1.
    Trial 12 pruned. Trial was pruned at iteration 2.
    Trial 13 pruned. Trial was pruned at iteration 1.
    Trial 14 pruned. Trial was pruned at iteration 47.
    Trial 15 pruned. Trial was pruned at iteration 1.
    Trial 16 pruned. Trial was pruned at iteration 1.
    Trial 17 finished with value: 0.958041958041958 and parameters: {'bagging_fraction': 0.8231361934235497, 'bagging_freq': 2, 'min_child_samples': 39}. Best is trial 1 with value: 0.986013986013986.
    Trial 18 pruned. Trial was pruned at iteration 1.
    Trial 19 pruned. Trial was pruned at iteration 1.
    Trial 20 pruned. Trial was pruned at iteration 0.
    Trial 21 pruned. Trial was pruned at iteration 0.
    Trial 22 pruned. Trial was pruned at iteration 0.
    Trial 23 pruned. Trial was pruned at iteration 0.
    Trial 24 finished with value: 0.9790209790209791 and parameters: {'bagging_fraction': 0.715564280537435, 'bagging_freq': 1, 'min_child_samples': 84}. Best is trial 1 with value: 0.986013986013986.
    Trial 25 pruned. Trial was pruned at iteration 58.
    Trial 26 pruned. Trial was pruned at iteration 0.
    Trial 27 pruned. Trial was pruned at iteration 0.
    Trial 28 pruned. Trial was pruned at iteration 0.
    Trial 29 pruned. Trial was pruned at iteration 0.
    Trial 30 pruned. Trial was pruned at iteration 0.
    Trial 31 pruned. Trial was pruned at iteration 0.
    Trial 32 pruned. Trial was pruned at iteration 0.
    Trial 33 pruned. Trial was pruned at iteration 0.
    Trial 34 pruned. Trial was pruned at iteration 0.
    Trial 35 pruned. Trial was pruned at iteration 0.
    Trial 36 pruned. Trial was pruned at iteration 0.
    Trial 37 pruned. Trial was pruned at iteration 0.
    Trial 38 pruned. Trial was pruned at iteration 0.
    Trial 39 pruned. Trial was pruned at iteration 1.
    Trial 40 pruned. Trial was pruned at iteration 1.
    Trial 41 pruned. Trial was pruned at iteration 0.
    Trial 42 pruned. Trial was pruned at iteration 1.
    Trial 43 pruned. Trial was pruned at iteration 0.
    Trial 44 pruned. Trial was pruned at iteration 0.
    Trial 45 pruned. Trial was pruned at iteration 1.
    Trial 46 pruned. Trial was pruned at iteration 34.
    Trial 47 finished with value: 0.9790209790209791 and parameters: {'bagging_fraction': 0.6363217393107216, 'bagging_freq': 0, 'min_child_samples': 84}. Best is trial 1 with value: 0.986013986013986.
    Trial 48 pruned. Trial was pruned at iteration 1.
    Trial 49 pruned. Trial was pruned at iteration 0.
    Trial 50 pruned. Trial was pruned at iteration 0.
    Trial 51 pruned. Trial was pruned at iteration 1.
    Trial 52 pruned. Trial was pruned at iteration 0.
    Trial 53 pruned. Trial was pruned at iteration 0.
    Trial 54 pruned. Trial was pruned at iteration 0.
    Trial 55 pruned. Trial was pruned at iteration 0.
    Trial 56 pruned. Trial was pruned at iteration 1.
    Trial 57 pruned. Trial was pruned at iteration 0.
    Trial 58 pruned. Trial was pruned at iteration 1.
    Trial 59 pruned. Trial was pruned at iteration 0.
    Trial 60 pruned. Trial was pruned at iteration 0.
    Trial 61 pruned. Trial was pruned at iteration 0.
    Trial 62 pruned. Trial was pruned at iteration 1.
    Trial 63 finished with value: 0.9790209790209791 and parameters: {'bagging_fraction': 0.9093174177600946, 'bagging_freq': 2, 'min_child_samples': 17}. Best is trial 1 with value: 0.986013986013986.
    Trial 64 finished with value: 0.986013986013986 and parameters: {'bagging_fraction': 0.9031919693250833, 'bagging_freq': 1, 'min_child_samples': 14}. Best is trial 1 with value: 0.986013986013986.
    Trial 65 pruned. Trial was pruned at iteration 0.
    Trial 66 pruned. Trial was pruned at iteration 0.
    Trial 67 pruned. Trial was pruned at iteration 1.
    Trial 68 pruned. Trial was pruned at iteration 0.
    Trial 69 pruned. Trial was pruned at iteration 1.
    Trial 70 pruned. Trial was pruned at iteration 0.
    Trial 71 finished with value: 0.9790209790209791 and parameters: {'bagging_fraction': 0.8008993867787653, 'bagging_freq': 2, 'min_child_samples': 18}. Best is trial 1 with value: 0.986013986013986.
    Trial 72 pruned. Trial was pruned at iteration 0.
    Trial 73 finished with value: 0.993006993006993 and parameters: {'bagging_fraction': 0.9135692816381003, 'bagging_freq': 1, 'min_child_samples': 27}. Best is trial 73 with value: 0.993006993006993.
    Trial 74 pruned. Trial was pruned at iteration 0.
    Trial 75 pruned. Trial was pruned at iteration 0.
    Trial 76 pruned. Trial was pruned at iteration 1.
    Trial 77 pruned. Trial was pruned at iteration 1.
    Trial 78 pruned. Trial was pruned at iteration 1.
    Trial 79 pruned. Trial was pruned at iteration 0.
    Trial 80 pruned. Trial was pruned at iteration 0.
    Trial 81 pruned. Trial was pruned at iteration 0.
    Trial 82 pruned. Trial was pruned at iteration 1.
    Trial 83 pruned. Trial was pruned at iteration 0.
    Trial 84 pruned. Trial was pruned at iteration 1.
    Trial 85 pruned. Trial was pruned at iteration 1.
    Trial 86 pruned. Trial was pruned at iteration 0.
    Trial 87 pruned. Trial was pruned at iteration 0.
    Trial 88 pruned. Trial was pruned at iteration 0.
    Trial 89 pruned. Trial was pruned at iteration 0.
    Trial 90 pruned. Trial was pruned at iteration 0.
    Trial 91 pruned. Trial was pruned at iteration 0.
    Trial 92 pruned. Trial was pruned at iteration 0.
    Trial 93 pruned. Trial was pruned at iteration 0.
    Trial 94 pruned. Trial was pruned at iteration 0.
    Trial 95 pruned. Trial was pruned at iteration 0.
    Trial 96 pruned. Trial was pruned at iteration 0.
    Trial 97 pruned. Trial was pruned at iteration 0.
    Trial 98 pruned. Trial was pruned at iteration 0.
    Trial 99 pruned. Trial was pruned at iteration 0.




.. GENERATED FROM PYTHON SOURCE LINES 102-116

.. _add_trial_tutorial:

----------------------------------------------------------------------
Second scenario: Have Optuna utilize already evaluated hyperparameters
----------------------------------------------------------------------

In this scenario, let's assume you have some out-of-box sets of hyperparameters and
you have already evaluated them but the results are not desirable so that you are thinking of
using Optuna.

Optuna has :func:`optuna.study.Study.add_trial` which lets you register those results
to Optuna and then Optuna will sample hyperparameters taking them into account.

In this section,  the ``objective`` is the same as the first scenario.

.. GENERATED FROM PYTHON SOURCE LINES 116-149

.. code-block:: default


    study = optuna.create_study(direction="maximize", pruner=optuna.pruners.MedianPruner())
    study.add_trial(
        optuna.trial.create_trial(
            params={
                "bagging_fraction": 1.0,
                "bagging_freq": 0,
                "min_child_samples": 20,
            },
            distributions={
                "bagging_fraction": optuna.distributions.FloatDistribution(0.4, 1.0 + 1e-12),
                "bagging_freq": optuna.distributions.IntDistribution(0, 7),
                "min_child_samples": optuna.distributions.IntDistribution(5, 100),
            },
            value=0.94,
        )
    )
    study.add_trial(
        optuna.trial.create_trial(
            params={
                "bagging_fraction": 0.75,
                "bagging_freq": 5,
                "min_child_samples": 20,
            },
            distributions={
                "bagging_fraction": optuna.distributions.FloatDistribution(0.4, 1.0 + 1e-12),
                "bagging_freq": optuna.distributions.IntDistribution(0, 7),
                "min_child_samples": optuna.distributions.IntDistribution(5, 100),
            },
            value=0.95,
        )
    )
    study.optimize(objective, n_trials=100, timeout=600)




.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    A new study created in memory with name: no-name-9f345df8-d56d-4efa-9823-b12398cd1638
    Trial 2 finished with value: 0.9440559440559441 and parameters: {'bagging_fraction': 0.668081063865479, 'bagging_freq': 5, 'min_child_samples': 70}. Best is trial 1 with value: 0.95.
    Trial 3 finished with value: 0.9790209790209791 and parameters: {'bagging_fraction': 0.4263705737357748, 'bagging_freq': 3, 'min_child_samples': 87}. Best is trial 3 with value: 0.9790209790209791.
    Trial 4 finished with value: 0.972027972027972 and parameters: {'bagging_fraction': 0.6882372048844847, 'bagging_freq': 1, 'min_child_samples': 25}. Best is trial 3 with value: 0.9790209790209791.
    Trial 5 finished with value: 0.9790209790209791 and parameters: {'bagging_fraction': 0.9874412856808806, 'bagging_freq': 6, 'min_child_samples': 88}. Best is trial 3 with value: 0.9790209790209791.
    Trial 6 pruned. Trial was pruned at iteration 10.
    Trial 7 pruned. Trial was pruned at iteration 0.
    Trial 8 pruned. Trial was pruned at iteration 19.
    Trial 9 pruned. Trial was pruned at iteration 0.
    Trial 10 pruned. Trial was pruned at iteration 0.
    Trial 11 pruned. Trial was pruned at iteration 0.
    Trial 12 pruned. Trial was pruned at iteration 0.
    Trial 13 finished with value: 0.972027972027972 and parameters: {'bagging_fraction': 0.8265352982849206, 'bagging_freq': 3, 'min_child_samples': 58}. Best is trial 3 with value: 0.9790209790209791.
    Trial 14 pruned. Trial was pruned at iteration 0.
    Trial 15 pruned. Trial was pruned at iteration 0.
    Trial 16 pruned. Trial was pruned at iteration 0.
    Trial 17 pruned. Trial was pruned at iteration 0.
    Trial 18 pruned. Trial was pruned at iteration 6.
    Trial 19 pruned. Trial was pruned at iteration 0.
    Trial 20 pruned. Trial was pruned at iteration 19.
    Trial 21 finished with value: 0.9790209790209791 and parameters: {'bagging_fraction': 0.7013992586052249, 'bagging_freq': 1, 'min_child_samples': 36}. Best is trial 3 with value: 0.9790209790209791.
    Trial 22 pruned. Trial was pruned at iteration 0.
    Trial 23 pruned. Trial was pruned at iteration 5.
    Trial 24 pruned. Trial was pruned at iteration 0.
    Trial 25 pruned. Trial was pruned at iteration 1.
    Trial 26 pruned. Trial was pruned at iteration 86.
    Trial 27 pruned. Trial was pruned at iteration 0.
    Trial 28 pruned. Trial was pruned at iteration 0.
    Trial 29 pruned. Trial was pruned at iteration 1.
    Trial 30 pruned. Trial was pruned at iteration 0.
    Trial 31 pruned. Trial was pruned at iteration 0.
    Trial 32 pruned. Trial was pruned at iteration 95.
    Trial 33 pruned. Trial was pruned at iteration 1.
    Trial 34 pruned. Trial was pruned at iteration 4.
    Trial 35 pruned. Trial was pruned at iteration 1.
    Trial 36 pruned. Trial was pruned at iteration 0.
    Trial 37 pruned. Trial was pruned at iteration 0.
    Trial 38 pruned. Trial was pruned at iteration 0.
    Trial 39 pruned. Trial was pruned at iteration 0.
    Trial 40 pruned. Trial was pruned at iteration 0.
    Trial 41 pruned. Trial was pruned at iteration 1.
    Trial 42 pruned. Trial was pruned at iteration 0.
    Trial 43 pruned. Trial was pruned at iteration 0.
    Trial 44 pruned. Trial was pruned at iteration 0.
    Trial 45 pruned. Trial was pruned at iteration 0.
    Trial 46 pruned. Trial was pruned at iteration 0.
    Trial 47 pruned. Trial was pruned at iteration 0.
    Trial 48 pruned. Trial was pruned at iteration 10.
    Trial 49 pruned. Trial was pruned at iteration 1.
    Trial 50 pruned. Trial was pruned at iteration 0.
    Trial 51 pruned. Trial was pruned at iteration 1.
    Trial 52 pruned. Trial was pruned at iteration 1.
    Trial 53 pruned. Trial was pruned at iteration 0.
    Trial 54 pruned. Trial was pruned at iteration 1.
    Trial 55 pruned. Trial was pruned at iteration 29.
    Trial 56 pruned. Trial was pruned at iteration 0.
    Trial 57 pruned. Trial was pruned at iteration 2.
    Trial 58 finished with value: 0.972027972027972 and parameters: {'bagging_fraction': 0.9952754170707239, 'bagging_freq': 7, 'min_child_samples': 17}. Best is trial 3 with value: 0.9790209790209791.
    Trial 59 pruned. Trial was pruned at iteration 0.
    Trial 60 pruned. Trial was pruned at iteration 0.
    Trial 61 pruned. Trial was pruned at iteration 1.
    Trial 62 finished with value: 0.972027972027972 and parameters: {'bagging_fraction': 0.9566138467313274, 'bagging_freq': 6, 'min_child_samples': 9}. Best is trial 3 with value: 0.9790209790209791.
    Trial 63 pruned. Trial was pruned at iteration 0.
    Trial 64 pruned. Trial was pruned at iteration 1.
    Trial 65 pruned. Trial was pruned at iteration 3.
    Trial 66 finished with value: 0.965034965034965 and parameters: {'bagging_fraction': 0.9720604022341277, 'bagging_freq': 6, 'min_child_samples': 17}. Best is trial 3 with value: 0.9790209790209791.
    Trial 67 pruned. Trial was pruned at iteration 0.
    Trial 68 finished with value: 0.993006993006993 and parameters: {'bagging_fraction': 0.9855448014687682, 'bagging_freq': 2, 'min_child_samples': 21}. Best is trial 68 with value: 0.993006993006993.
    Trial 69 pruned. Trial was pruned at iteration 1.
    Trial 70 pruned. Trial was pruned at iteration 1.
    Trial 71 finished with value: 0.951048951048951 and parameters: {'bagging_fraction': 0.9751600540332035, 'bagging_freq': 3, 'min_child_samples': 9}. Best is trial 68 with value: 0.993006993006993.
    Trial 72 pruned. Trial was pruned at iteration 0.
    Trial 73 pruned. Trial was pruned at iteration 0.
    Trial 74 finished with value: 0.993006993006993 and parameters: {'bagging_fraction': 0.9162555704403906, 'bagging_freq': 1, 'min_child_samples': 22}. Best is trial 68 with value: 0.993006993006993.
    Trial 75 pruned. Trial was pruned at iteration 0.
    Trial 76 finished with value: 0.9790209790209791 and parameters: {'bagging_fraction': 0.979578586704233, 'bagging_freq': 1, 'min_child_samples': 22}. Best is trial 68 with value: 0.993006993006993.
    Trial 77 pruned. Trial was pruned at iteration 0.
    Trial 78 pruned. Trial was pruned at iteration 0.
    Trial 79 pruned. Trial was pruned at iteration 0.
    Trial 80 pruned. Trial was pruned at iteration 3.
    Trial 81 pruned. Trial was pruned at iteration 1.
    Trial 82 pruned. Trial was pruned at iteration 0.
    Trial 83 pruned. Trial was pruned at iteration 0.
    Trial 84 pruned. Trial was pruned at iteration 0.
    Trial 85 pruned. Trial was pruned at iteration 0.
    Trial 86 pruned. Trial was pruned at iteration 0.
    Trial 87 pruned. Trial was pruned at iteration 0.
    Trial 88 pruned. Trial was pruned at iteration 0.
    Trial 89 pruned. Trial was pruned at iteration 0.
    Trial 90 pruned. Trial was pruned at iteration 0.
    Trial 91 pruned. Trial was pruned at iteration 0.
    Trial 92 pruned. Trial was pruned at iteration 0.
    Trial 93 pruned. Trial was pruned at iteration 0.
    Trial 94 pruned. Trial was pruned at iteration 8.
    Trial 95 pruned. Trial was pruned at iteration 0.
    Trial 96 pruned. Trial was pruned at iteration 0.
    Trial 97 pruned. Trial was pruned at iteration 0.
    Trial 98 pruned. Trial was pruned at iteration 3.
    Trial 99 pruned. Trial was pruned at iteration 0.
    Trial 100 pruned. Trial was pruned at iteration 1.
    Trial 101 pruned. Trial was pruned at iteration 3.





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  7.663 seconds)


.. _sphx_glr_download_tutorial_20_recipes_008_specify_params.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example


    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: 008_specify_params.py <008_specify_params.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: 008_specify_params.ipynb <008_specify_params.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
