
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "tutorial/20_recipes/002_multi_objective.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_tutorial_20_recipes_002_multi_objective.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_tutorial_20_recipes_002_multi_objective.py:


.. _multi_objective:

Multi-objective Optimization with Optuna
========================================

This tutorial showcases Optuna's multi-objective optimization feature by
optimizing the validation accuracy of Fashion MNIST dataset and the FLOPS of the model implemented in PyTorch.

We use `fvcore <https://github.com/facebookresearch/fvcore>`_ to measure FLOPS.

.. GENERATED FROM PYTHON SOURCE LINES 12-74

.. code-block:: default


    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    import torchvision
    from fvcore.nn import FlopCountAnalysis

    import optuna


    DEVICE = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
    DIR = ".."
    BATCHSIZE = 128
    N_TRAIN_EXAMPLES = BATCHSIZE * 30
    N_VALID_EXAMPLES = BATCHSIZE * 10


    def define_model(trial):
        n_layers = trial.suggest_int("n_layers", 1, 3)
        layers = []

        in_features = 28 * 28
        for i in range(n_layers):
            out_features = trial.suggest_int("n_units_l{}".format(i), 4, 128)
            layers.append(nn.Linear(in_features, out_features))
            layers.append(nn.ReLU())
            p = trial.suggest_float("dropout_{}".format(i), 0.2, 0.5)
            layers.append(nn.Dropout(p))

            in_features = out_features

        layers.append(nn.Linear(in_features, 10))
        layers.append(nn.LogSoftmax(dim=1))

        return nn.Sequential(*layers)


    # Defines training and evaluation.
    def train_model(model, optimizer, train_loader):
        model.train()
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.view(-1, 28 * 28).to(DEVICE), target.to(DEVICE)
            optimizer.zero_grad()
            F.nll_loss(model(data), target).backward()
            optimizer.step()


    def eval_model(model, valid_loader):
        model.eval()
        correct = 0
        with torch.no_grad():
            for batch_idx, (data, target) in enumerate(valid_loader):
                data, target = data.view(-1, 28 * 28).to(DEVICE), target.to(DEVICE)
                pred = model(data).argmax(dim=1, keepdim=True)
                correct += pred.eq(target.view_as(pred)).sum().item()

        accuracy = correct / N_VALID_EXAMPLES

        flops = FlopCountAnalysis(model, inputs=(torch.randn(1, 28 * 28).to(DEVICE),)).total()
        return flops, accuracy









.. GENERATED FROM PYTHON SOURCE LINES 75-77

Define multi-objective objective function.
Objectives are FLOPS and accuracy.

.. GENERATED FROM PYTHON SOURCE LINES 77-107

.. code-block:: default

    def objective(trial):
        train_dataset = torchvision.datasets.FashionMNIST(
            DIR, train=True, download=True, transform=torchvision.transforms.ToTensor()
        )
        train_loader = torch.utils.data.DataLoader(
            torch.utils.data.Subset(train_dataset, list(range(N_TRAIN_EXAMPLES))),
            batch_size=BATCHSIZE,
            shuffle=True,
        )

        val_dataset = torchvision.datasets.FashionMNIST(
            DIR, train=False, transform=torchvision.transforms.ToTensor()
        )
        val_loader = torch.utils.data.DataLoader(
            torch.utils.data.Subset(val_dataset, list(range(N_VALID_EXAMPLES))),
            batch_size=BATCHSIZE,
            shuffle=True,
        )
        model = define_model(trial).to(DEVICE)

        optimizer = torch.optim.Adam(
            model.parameters(), trial.suggest_float("lr", 1e-5, 1e-1, log=True)
        )

        for epoch in range(10):
            train_model(model, optimizer, train_loader)
        flops, accuracy = eval_model(model, val_loader)
        return flops, accuracy









.. GENERATED FROM PYTHON SOURCE LINES 108-115

Run multi-objective optimization
--------------------------------

If your optimization problem is multi-objective,
Optuna assumes that you will specify the optimization direction for each objective.
Specifically, in this example, we want to minimize the FLOPS (we want a faster model)
and maximize the accuracy. So we set ``directions`` to ``["minimize", "maximize"]``.

.. GENERATED FROM PYTHON SOURCE LINES 115-121

.. code-block:: default

    study = optuna.create_study(directions=["minimize", "maximize"])
    study.optimize(objective, n_trials=30, timeout=300)

    print("Number of finished trials: ", len(study.trials))






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz
    Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ../FashionMNIST/raw/train-images-idx3-ubyte.gz
      0%|          | 0/26421880 [00:00<?, ?it/s]      0%|          | 13312/26421880 [00:00<03:49, 115288.52it/s]      0%|          | 43008/26421880 [00:00<02:14, 196476.30it/s]      0%|          | 103424/26421880 [00:00<01:16, 342020.72it/s]      1%|          | 223232/26421880 [00:00<00:43, 608898.16it/s]      2%|1         | 463872/26421880 [00:00<00:23, 1127772.62it/s]      4%|3         | 945152/26421880 [00:00<00:11, 2137161.77it/s]      7%|7         | 1907712/26421880 [00:00<00:05, 4110600.08it/s]     15%|#4        | 3831808/26421880 [00:00<00:02, 8006501.76it/s]     26%|##5       | 6758400/26421880 [00:01<00:01, 13265804.75it/s]     36%|###5      | 9508864/26421880 [00:01<00:01, 16375664.41it/s]     46%|####6     | 12176384/26421880 [00:01<00:00, 18308166.36it/s]     57%|#####6    | 14930944/26421880 [00:01<00:00, 19868845.30it/s]     67%|######7   | 17718272/26421880 [00:01<00:00, 21035937.03it/s]     78%|#######7  | 20598784/26421880 [00:01<00:00, 22077206.97it/s]     89%|########8 | 23447552/26421880 [00:01<00:00, 22698992.65it/s]     99%|#########8| 26064896/26421880 [00:01<00:00, 22557805.46it/s]    26422272it [00:01, 14022240.77it/s]                              
    Extracting ../FashionMNIST/raw/train-images-idx3-ubyte.gz to ../FashionMNIST/raw

    Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz
    Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ../FashionMNIST/raw/train-labels-idx1-ubyte.gz
      0%|          | 0/29515 [00:00<?, ?it/s]     49%|####8     | 14336/29515 [00:00<00:00, 113243.64it/s]    29696it [00:00, 233319.82it/s]                           
    Extracting ../FashionMNIST/raw/train-labels-idx1-ubyte.gz to ../FashionMNIST/raw

    Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz
    Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ../FashionMNIST/raw/t10k-images-idx3-ubyte.gz
      0%|          | 0/4422102 [00:00<?, ?it/s]      0%|          | 14336/4422102 [00:00<00:36, 121579.44it/s]      1%|          | 44032/4422102 [00:00<00:22, 197396.08it/s]      2%|2         | 104448/4422102 [00:00<00:12, 339675.66it/s]      5%|5         | 224256/4422102 [00:00<00:06, 603931.97it/s]     11%|#         | 464896/4422102 [00:00<00:03, 1116455.48it/s]     21%|##1       | 946176/4422102 [00:00<00:01, 2113129.30it/s]     43%|####3     | 1908736/4422102 [00:00<00:00, 4068718.67it/s]     87%|########6 | 3832832/4422102 [00:00<00:00, 7925205.33it/s]    4422656it [00:00, 4632594.29it/s]                             
    Extracting ../FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ../FashionMNIST/raw

    Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz
    Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ../FashionMNIST/raw/t10k-labels-idx1-ubyte.gz
      0%|          | 0/5148 [00:00<?, ?it/s]    6144it [00:00, 28987405.82it/s]         
    Extracting ../FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ../FashionMNIST/raw

    Unsupported operator aten::log_softmax encountered 1 time(s)
    Unsupported operator aten::log_softmax encountered 1 time(s)
    Unsupported operator aten::log_softmax encountered 1 time(s)
    Unsupported operator aten::log_softmax encountered 1 time(s)
    Unsupported operator aten::log_softmax encountered 1 time(s)
    Unsupported operator aten::log_softmax encountered 1 time(s)
    Unsupported operator aten::log_softmax encountered 1 time(s)
    Unsupported operator aten::log_softmax encountered 1 time(s)
    Unsupported operator aten::log_softmax encountered 1 time(s)
    Unsupported operator aten::log_softmax encountered 1 time(s)
    Unsupported operator aten::log_softmax encountered 1 time(s)
    Unsupported operator aten::log_softmax encountered 1 time(s)
    Unsupported operator aten::log_softmax encountered 1 time(s)
    Unsupported operator aten::log_softmax encountered 1 time(s)
    Unsupported operator aten::log_softmax encountered 1 time(s)
    Unsupported operator aten::log_softmax encountered 1 time(s)
    Unsupported operator aten::log_softmax encountered 1 time(s)
    Unsupported operator aten::log_softmax encountered 1 time(s)
    Unsupported operator aten::log_softmax encountered 1 time(s)
    Unsupported operator aten::log_softmax encountered 1 time(s)
    Unsupported operator aten::log_softmax encountered 1 time(s)
    Unsupported operator aten::log_softmax encountered 1 time(s)
    Unsupported operator aten::log_softmax encountered 1 time(s)
    Unsupported operator aten::log_softmax encountered 1 time(s)
    Unsupported operator aten::log_softmax encountered 1 time(s)
    Unsupported operator aten::log_softmax encountered 1 time(s)
    Unsupported operator aten::log_softmax encountered 1 time(s)
    Unsupported operator aten::log_softmax encountered 1 time(s)
    Unsupported operator aten::log_softmax encountered 1 time(s)
    Unsupported operator aten::log_softmax encountered 1 time(s)
    Number of finished trials:  30




.. GENERATED FROM PYTHON SOURCE LINES 122-123

Check trials on Pareto front visually.

.. GENERATED FROM PYTHON SOURCE LINES 123-126

.. code-block:: default

    optuna.visualization.plot_pareto_front(study, target_names=["FLOPS", "accuracy"])







.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
            <script charset="utf-8" src="https://cdn.plot.ly/plotly-2.20.0.min.js"></script>                <div id="efed19ae-d71f-4163-8f3a-3044b8d659f0" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("efed19ae-d71f-4163-8f3a-3044b8d659f0")) {                    Plotly.newPlot(                        "efed19ae-d71f-4163-8f3a-3044b8d659f0",                        [{"hovertemplate":"%{text}<extra>Trial</extra>","marker":{"color":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,22,24,25,26,27,28,29],"colorbar":{"title":{"text":"Trial"}},"colorscale":[[0.0,"rgb(247,251,255)"],[0.125,"rgb(222,235,247)"],[0.25,"rgb(198,219,239)"],[0.375,"rgb(158,202,225)"],[0.5,"rgb(107,174,214)"],[0.625,"rgb(66,146,198)"],[0.75,"rgb(33,113,181)"],[0.875,"rgb(8,81,156)"],[1.0,"rgb(8,48,107)"]],"line":{"color":"Grey","width":0.5}},"mode":"markers","showlegend":false,"text":["{<br>  \"number\": 0,<br>  \"values\": [<br>    41472.0,<br>    0.7125<br>  ],<br>  \"params\": {<br>    \"n_layers\": 3,<br>    \"n_units_l0\": 43,<br>    \"dropout_0\": 0.36234560086903334,<br>    \"n_units_l1\": 95,<br>    \"dropout_1\": 0.42492613350036157,<br>    \"n_units_l2\": 35,<br>    \"dropout_2\": 0.31338795951812937,<br>    \"lr\": 0.006747310353372207<br>  }<br>}","{<br>  \"number\": 1,<br>  \"values\": [<br>    60344.0,<br>    0.56015625<br>  ],<br>  \"params\": {<br>    \"n_layers\": 1,<br>    \"n_units_l0\": 76,<br>    \"dropout_0\": 0.4204316847033528,<br>    \"lr\": 1.0144081780862311e-05<br>  }<br>}","{<br>  \"number\": 2,<br>  \"values\": [<br>    38136.0,<br>    0.73515625<br>  ],<br>  \"params\": {<br>    \"n_layers\": 3,<br>    \"n_units_l0\": 44,<br>    \"dropout_0\": 0.22201852434436709,<br>    \"n_units_l1\": 14,<br>    \"dropout_1\": 0.4281718452268283,<br>    \"n_units_l2\": 126,<br>    \"dropout_2\": 0.3582680085209564,<br>    \"lr\": 0.002495443300185707<br>  }<br>}","{<br>  \"number\": 3,<br>  \"values\": [<br>    33520.0,<br>    0.8<br>  ],<br>  \"params\": {<br>    \"n_layers\": 2,<br>    \"n_units_l0\": 37,<br>    \"dropout_0\": 0.24342645093821275,<br>    \"n_units_l1\": 96,<br>    \"dropout_1\": 0.35367131847773314,<br>    \"lr\": 0.015303250978280484<br>  }<br>}","{<br>  \"number\": 4,<br>  \"values\": [<br>    37894.0,<br>    0.1953125<br>  ],<br>  \"params\": {<br>    \"n_layers\": 3,<br>    \"n_units_l0\": 41,<br>    \"dropout_0\": 0.4885543168761145,<br>    \"n_units_l1\": 70,<br>    \"dropout_1\": 0.40483849275227757,<br>    \"n_units_l2\": 36,<br>    \"dropout_2\": 0.4581297493612015,<br>    \"lr\": 2.0067874784475854e-05<br>  }<br>}","{<br>  \"number\": 5,<br>  \"values\": [<br>    72485.0,<br>    0.09296875<br>  ],<br>  \"params\": {<br>    \"n_layers\": 3,<br>    \"n_units_l0\": 84,<br>    \"dropout_0\": 0.24256053047801338,<br>    \"n_units_l1\": 67,<br>    \"dropout_1\": 0.38588212232477304,<br>    \"n_units_l2\": 13,<br>    \"dropout_2\": 0.4645043778432198,<br>    \"lr\": 0.07248074657222513<br>  }<br>}","{<br>  \"number\": 6,<br>  \"values\": [<br>    103964.0,<br>    0.44765625<br>  ],<br>  \"params\": {<br>    \"n_layers\": 2,<br>    \"n_units_l0\": 122,<br>    \"dropout_0\": 0.2878223569337163,<br>    \"n_units_l1\": 63,<br>    \"dropout_1\": 0.2568225501055329,<br>    \"lr\": 1.5404570266551703e-05<br>  }<br>}","{<br>  \"number\": 7,<br>  \"values\": [<br>    27360.0,<br>    0.74140625<br>  ],<br>  \"params\": {<br>    \"n_layers\": 3,<br>    \"n_units_l0\": 20,<br>    \"dropout_0\": 0.39863830516099347,<br>    \"n_units_l1\": 78,<br>    \"dropout_1\": 0.2087136602873756,<br>    \"n_units_l2\": 115,<br>    \"dropout_2\": 0.2745150760495395,<br>    \"lr\": 0.002325080148045474<br>  }<br>}","{<br>  \"number\": 8,<br>  \"values\": [<br>    32298.0,<br>    0.1015625<br>  ],<br>  \"params\": {<br>    \"n_layers\": 3,<br>    \"n_units_l0\": 35,<br>    \"dropout_0\": 0.31162509583749953,<br>    \"n_units_l1\": 74,<br>    \"dropout_1\": 0.37497788717485503,<br>    \"n_units_l2\": 27,<br>    \"dropout_2\": 0.26248529696094264,<br>    \"lr\": 0.0910810514279263<br>  }<br>}","{<br>  \"number\": 9,<br>  \"values\": [<br>    57474.0,<br>    0.26640625<br>  ],<br>  \"params\": {<br>    \"n_layers\": 3,<br>    \"n_units_l0\": 61,<br>    \"dropout_0\": 0.2116582190391876,<br>    \"n_units_l1\": 104,<br>    \"dropout_1\": 0.2376669732386899,<br>    \"n_units_l2\": 29,<br>    \"dropout_2\": 0.33372310974614405,<br>    \"lr\": 0.07486648206700261<br>  }<br>}","{<br>  \"number\": 10,<br>  \"values\": [<br>    80700.0,<br>    0.65859375<br>  ],<br>  \"params\": {<br>    \"n_layers\": 3,<br>    \"n_units_l0\": 89,<br>    \"dropout_0\": 0.2800921162755195,<br>    \"n_units_l1\": 56,<br>    \"dropout_1\": 0.44430384986196564,<br>    \"n_units_l2\": 90,<br>    \"dropout_2\": 0.24303393453803224,<br>    \"lr\": 0.00027376421110925607<br>  }<br>}","{<br>  \"number\": 11,<br>  \"values\": [<br>    35730.0,<br>    0.6328125<br>  ],<br>  \"params\": {<br>    \"n_layers\": 1,<br>    \"n_units_l0\": 45,<br>    \"dropout_0\": 0.35376941268477263,<br>    \"lr\": 7.998443920659069e-05<br>  }<br>}","{<br>  \"number\": 12,<br>  \"values\": [<br>    96074.0,<br>    0.65078125<br>  ],<br>  \"params\": {<br>    \"n_layers\": 1,<br>    \"n_units_l0\": 121,<br>    \"dropout_0\": 0.452523727603545,<br>    \"lr\": 5.262845143594697e-05<br>  }<br>}","{<br>  \"number\": 13,<br>  \"values\": [<br>    69145.0,<br>    0.81484375<br>  ],<br>  \"params\": {<br>    \"n_layers\": 2,<br>    \"n_units_l0\": 79,<br>    \"dropout_0\": 0.22123189353883407,<br>    \"n_units_l1\": 81,<br>    \"dropout_1\": 0.2370187057128619,<br>    \"lr\": 0.003063773986593654<br>  }<br>}","{<br>  \"number\": 14,<br>  \"values\": [<br>    49233.0,<br>    0.23125<br>  ],<br>  \"params\": {<br>    \"n_layers\": 3,<br>    \"n_units_l0\": 62,<br>    \"dropout_0\": 0.4612893239944525,<br>    \"n_units_l1\": 5,<br>    \"dropout_1\": 0.48483716950974753,<br>    \"n_units_l2\": 21,<br>    \"dropout_2\": 0.280670520227765,<br>    \"lr\": 0.00017814898556513798<br>  }<br>}","{<br>  \"number\": 15,<br>  \"values\": [<br>    27420.0,<br>    0.59375<br>  ],<br>  \"params\": {<br>    \"n_layers\": 2,<br>    \"n_units_l0\": 33,<br>    \"dropout_0\": 0.4801175904495347,<br>    \"n_units_l1\": 36,<br>    \"dropout_1\": 0.4208384180779068,<br>    \"lr\": 9.840688394288934e-05<br>  }<br>}","{<br>  \"number\": 16,<br>  \"values\": [<br>    30192.0,<br>    0.321875<br>  ],<br>  \"params\": {<br>    \"n_layers\": 3,<br>    \"n_units_l0\": 35,<br>    \"dropout_0\": 0.47207506382704995,<br>    \"n_units_l1\": 23,<br>    \"dropout_1\": 0.40799262109259615,<br>    \"n_units_l2\": 59,<br>    \"dropout_2\": 0.45612668791543615,<br>    \"lr\": 9.50055218890261e-05<br>  }<br>}","{<br>  \"number\": 17,<br>  \"values\": [<br>    88740.0,<br>    0.7953125<br>  ],<br>  \"params\": {<br>    \"n_layers\": 2,<br>    \"n_units_l0\": 100,<br>    \"dropout_0\": 0.4601394657552748,<br>    \"n_units_l1\": 94,<br>    \"dropout_1\": 0.33928006694666196,<br>    \"lr\": 0.0008684845701983447<br>  }<br>}","{<br>  \"number\": 22,<br>  \"values\": [<br>    55580.0,<br>    0.74375<br>  ],<br>  \"params\": {<br>    \"n_layers\": 1,<br>    \"n_units_l0\": 70,<br>    \"dropout_0\": 0.37849152183555707,<br>    \"lr\": 0.0002783699880821494<br>  }<br>}","{<br>  \"number\": 24,<br>  \"values\": [<br>    33396.0,<br>    0.7703125<br>  ],<br>  \"params\": {<br>    \"n_layers\": 2,<br>    \"n_units_l0\": 42,<br>    \"dropout_0\": 0.34013805241030126,<br>    \"n_units_l1\": 9,<br>    \"dropout_1\": 0.2133610054695257,<br>    \"lr\": 0.006738921812686957<br>  }<br>}","{<br>  \"number\": 25,<br>  \"values\": [<br>    44360.0,<br>    0.478125<br>  ],<br>  \"params\": {<br>    \"n_layers\": 2,<br>    \"n_units_l0\": 48,<br>    \"dropout_0\": 0.46620345074994457,<br>    \"n_units_l1\": 116,<br>    \"dropout_1\": 0.45638162793289627,<br>    \"lr\": 0.0437177691792032<br>  }<br>}","{<br>  \"number\": 26,<br>  \"values\": [<br>    49708.0,<br>    0.5921875<br>  ],<br>  \"params\": {<br>    \"n_layers\": 3,<br>    \"n_units_l0\": 50,<br>    \"dropout_0\": 0.4790356437544694,<br>    \"n_units_l1\": 118,<br>    \"dropout_1\": 0.4051802404276061,<br>    \"n_units_l2\": 36,<br>    \"dropout_2\": 0.41023581131482056,<br>    \"lr\": 0.0002187848398183355<br>  }<br>}","{<br>  \"number\": 27,<br>  \"values\": [<br>    62726.0,<br>    0.71328125<br>  ],<br>  \"params\": {<br>    \"n_layers\": 1,<br>    \"n_units_l0\": 79,<br>    \"dropout_0\": 0.2232586832623874,<br>    \"lr\": 0.00014090089072279555<br>  }<br>}","{<br>  \"number\": 28,<br>  \"values\": [<br>    115195.0,<br>    0.809375<br>  ],<br>  \"params\": {<br>    \"n_layers\": 3,<br>    \"n_units_l0\": 125,<br>    \"dropout_0\": 0.3006512279556349,<br>    \"n_units_l1\": 75,<br>    \"dropout_1\": 0.4492109212036593,<br>    \"n_units_l2\": 92,<br>    \"dropout_2\": 0.4118310329188315,<br>    \"lr\": 0.00203455610577538<br>  }<br>}","{<br>  \"number\": 29,<br>  \"values\": [<br>    32554.0,<br>    0.73671875<br>  ],<br>  \"params\": {<br>    \"n_layers\": 3,<br>    \"n_units_l0\": 26,<br>    \"dropout_0\": 0.21412641299338148,<br>    \"n_units_l1\": 103,<br>    \"dropout_1\": 0.47560532305313435,<br>    \"n_units_l2\": 84,<br>    \"dropout_2\": 0.3266986520206008,<br>    \"lr\": 0.0006149140730025004<br>  }<br>}"],"x":[41472.0,60344.0,38136.0,33520.0,37894.0,72485.0,103964.0,27360.0,32298.0,57474.0,80700.0,35730.0,96074.0,69145.0,49233.0,27420.0,30192.0,88740.0,55580.0,33396.0,44360.0,49708.0,62726.0,115195.0,32554.0],"y":[0.7125,0.56015625,0.73515625,0.8,0.1953125,0.09296875,0.44765625,0.74140625,0.1015625,0.26640625,0.65859375,0.6328125,0.65078125,0.81484375,0.23125,0.59375,0.321875,0.7953125,0.74375,0.7703125,0.478125,0.5921875,0.71328125,0.809375,0.73671875],"type":"scatter"},{"hovertemplate":"%{text}<extra>Best Trial</extra>","marker":{"color":[18,19,20,21,23],"colorbar":{"title":{"text":"Best Trial"},"x":1.1,"xpad":40},"colorscale":[[0.0,"rgb(255,245,240)"],[0.125,"rgb(254,224,210)"],[0.25,"rgb(252,187,161)"],[0.375,"rgb(252,146,114)"],[0.5,"rgb(251,106,74)"],[0.625,"rgb(239,59,44)"],[0.75,"rgb(203,24,29)"],[0.875,"rgb(165,15,21)"],[1.0,"rgb(103,0,13)"]],"line":{"color":"Grey","width":0.5}},"mode":"markers","showlegend":false,"text":["{<br>  \"number\": 18,<br>  \"values\": [<br>    12024.0,<br>    0.6265625<br>  ],<br>  \"params\": {<br>    \"n_layers\": 3,<br>    \"n_units_l0\": 4,<br>    \"dropout_0\": 0.2866523241364911,<br>    \"n_units_l1\": 62,<br>    \"dropout_1\": 0.2656500102034613,<br>    \"n_units_l2\": 120,<br>    \"dropout_2\": 0.3791540330221792,<br>    \"lr\": 0.0015287801367130528<br>  }<br>}","{<br>  \"number\": 19,<br>  \"values\": [<br>    31204.0,<br>    0.815625<br>  ],<br>  \"params\": {<br>    \"n_layers\": 2,<br>    \"n_units_l0\": 33,<br>    \"dropout_0\": 0.3858085160088696,<br>    \"n_units_l1\": 124,<br>    \"dropout_1\": 0.20692043221855963,<br>    \"lr\": 0.007464338159560122<br>  }<br>}","{<br>  \"number\": 20,<br>  \"values\": [<br>    26200.0,<br>    0.73046875<br>  ],<br>  \"params\": {<br>    \"n_layers\": 2,<br>    \"n_units_l0\": 30,<br>    \"dropout_0\": 0.4856135270064511,<br>    \"n_units_l1\": 67,<br>    \"dropout_1\": 0.20631923604053953,<br>    \"lr\": 0.00045395858977939006<br>  }<br>}","{<br>  \"number\": 21,<br>  \"values\": [<br>    84958.0,<br>    0.83984375<br>  ],<br>  \"params\": {<br>    \"n_layers\": 1,<br>    \"n_units_l0\": 107,<br>    \"dropout_0\": 0.228942594473058,<br>    \"lr\": 0.0028317744386418724<br>  }<br>}","{<br>  \"number\": 23,<br>  \"values\": [<br>    26202.0,<br>    0.80625<br>  ],<br>  \"params\": {<br>    \"n_layers\": 1,<br>    \"n_units_l0\": 33,<br>    \"dropout_0\": 0.41110006916490793,<br>    \"lr\": 0.01188655547758307<br>  }<br>}"],"x":[12024.0,31204.0,26200.0,84958.0,26202.0],"y":[0.6265625,0.815625,0.73046875,0.83984375,0.80625],"type":"scatter"}],                        {"title":{"text":"Pareto-front Plot"},"xaxis":{"title":{"text":"FLOPS"}},"yaxis":{"title":{"text":"accuracy"}},"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}}},                        {"responsive": true}                    )                };                            </script>        </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 127-130

Fetch the list of trials on the Pareto front with :attr:`~optuna.study.Study.best_trials`.

For example, the following code shows the number of trials on the Pareto front and picks the trial with the highest accuracy.

.. GENERATED FROM PYTHON SOURCE LINES 130-139

.. code-block:: default


    print(f"Number of trials on the Pareto front: {len(study.best_trials)}")

    trial_with_highest_accuracy = max(study.best_trials, key=lambda t: t.values[1])
    print(f"Trial with highest accuracy: ")
    print(f"\tnumber: {trial_with_highest_accuracy.number}")
    print(f"\tparams: {trial_with_highest_accuracy.params}")
    print(f"\tvalues: {trial_with_highest_accuracy.values}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Number of trials on the Pareto front: 5
    Trial with highest accuracy: 
            number: 21
            params: {'n_layers': 1, 'n_units_l0': 107, 'dropout_0': 0.228942594473058, 'lr': 0.0028317744386418724}
            values: [84958.0, 0.83984375]




.. GENERATED FROM PYTHON SOURCE LINES 140-141

Learn which hyperparameters are affecting the flops most with hyperparameter importance.

.. GENERATED FROM PYTHON SOURCE LINES 141-144

.. code-block:: default

    optuna.visualization.plot_param_importances(
        study, target=lambda t: t.values[0], target_name="flops"
    )





.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
            <script charset="utf-8" src="https://cdn.plot.ly/plotly-2.20.0.min.js"></script>                <div id="4fb71e81-59c3-41b1-a0c3-e2f7d43fa01b" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("4fb71e81-59c3-41b1-a0c3-e2f7d43fa01b")) {                    Plotly.newPlot(                        "4fb71e81-59c3-41b1-a0c3-e2f7d43fa01b",                        [{"cliponaxis":false,"hovertemplate":["lr (FloatDistribution): 0.0010894792501818062<extra></extra>","n_layers (IntDistribution): 0.001498673015227809<extra></extra>","dropout_0 (FloatDistribution): 0.0067788598863080494<extra></extra>","n_units_l0 (IntDistribution): 0.9906329878482825<extra></extra>"],"marker":{"color":"rgb(66,146,198)"},"orientation":"h","text":["<0.01","<0.01","<0.01","0.99"],"textposition":"outside","x":[0.0010894792501818062,0.001498673015227809,0.0067788598863080494,0.9906329878482825],"y":["lr","n_layers","dropout_0","n_units_l0"],"type":"bar"}],                        {"showlegend":false,"title":{"text":"Hyperparameter Importances"},"xaxis":{"title":{"text":"Importance for flops"}},"yaxis":{"title":{"text":"Hyperparameter"}},"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}}},                        {"responsive": true}                    )                };                            </script>        </div>
    </div>
    <br />
    <br />


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 3 minutes  18.706 seconds)


.. _sphx_glr_download_tutorial_20_recipes_002_multi_objective.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example




    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: 002_multi_objective.py <002_multi_objective.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: 002_multi_objective.ipynb <002_multi_objective.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
