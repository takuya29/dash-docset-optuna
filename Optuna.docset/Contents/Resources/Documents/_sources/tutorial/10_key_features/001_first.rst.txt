
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "tutorial/10_key_features/001_first.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_tutorial_10_key_features_001_first.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_tutorial_10_key_features_001_first.py:


.. _first:

Lightweight, versatile, and platform agnostic architecture
==========================================================

Optuna is entirely written in Python and has few dependencies.
This means that we can quickly move to the real example once you get interested in Optuna.


Quadratic Function Example
--------------------------

Usually, Optuna is used to optimize hyperparameters, but as an example,
let's optimize a simple quadratic function: :math:`(x - 2)^2`.

.. GENERATED FROM PYTHON SOURCE LINES 19-20

First of all, import :mod:`optuna`.

.. GENERATED FROM PYTHON SOURCE LINES 20-24

.. code-block:: Python


    import optuna









.. GENERATED FROM PYTHON SOURCE LINES 25-26

In optuna, conventionally functions to be optimized are named `objective`.

.. GENERATED FROM PYTHON SOURCE LINES 26-33

.. code-block:: Python



    def objective(trial):
        x = trial.suggest_float("x", -10, 10)
        return (x - 2) ** 2









.. GENERATED FROM PYTHON SOURCE LINES 34-49

This function returns the value of :math:`(x - 2)^2`. Our goal is to find the value of ``x``
that minimizes the output of the ``objective`` function. This is the "optimization."
During the optimization, Optuna repeatedly calls and evaluates the objective function with
different values of ``x``.

A :class:`~optuna.trial.Trial` object corresponds to a single execution of the objective
function and is internally instantiated upon each invocation of the function.

The `suggest` APIs (for example, :func:`~optuna.trial.Trial.suggest_float`) are called
inside the objective function to obtain parameters for a trial.
:func:`~optuna.trial.Trial.suggest_float` selects parameters uniformly within the range
provided. In our example, from :math:`-10` to :math:`10`.

To start the optimization, we create a study object and pass the objective function to method
:func:`~optuna.study.Study.optimize` as follows.

.. GENERATED FROM PYTHON SOURCE LINES 49-54

.. code-block:: Python


    study = optuna.create_study()
    study.optimize(objective, n_trials=100)









.. GENERATED FROM PYTHON SOURCE LINES 55-56

You can get the best parameter as follows.

.. GENERATED FROM PYTHON SOURCE LINES 56-61

.. code-block:: Python


    best_params = study.best_params
    found_x = best_params["x"]
    print("Found x: {}, (x - 2)^2: {}".format(found_x, (found_x - 2) ** 2))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Found x: 2.0050895910749604, (x - 2)^2: 2.590393731031688e-05




.. GENERATED FROM PYTHON SOURCE LINES 62-63

We can see that the ``x`` value found by Optuna is close to the optimal value of ``2``.

.. GENERATED FROM PYTHON SOURCE LINES 65-69

.. note::
    When used to search for hyperparameters in machine learning,
    usually the objective function would return the loss or accuracy
    of the model.

.. GENERATED FROM PYTHON SOURCE LINES 72-84

Study Object
------------

Let us clarify the terminology in Optuna as follows:

* **Trial**: A single call of the objective function
* **Study**: An optimization session, which is a set of trials
* **Parameter**: A variable whose value is to be optimized, such as ``x`` in the above example

In Optuna, we use the study object to manage optimization.
Method :func:`~optuna.study.create_study` returns a study object.
A study object has useful properties for analyzing the optimization outcome.

.. GENERATED FROM PYTHON SOURCE LINES 86-87

To get the dictionary of parameter name and parameter values:

.. GENERATED FROM PYTHON SOURCE LINES 87-91

.. code-block:: Python



    study.best_params





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    {'x': 2.0050895910749604}



.. GENERATED FROM PYTHON SOURCE LINES 92-93

To get the best observed value of the objective function:

.. GENERATED FROM PYTHON SOURCE LINES 93-97

.. code-block:: Python


    study.best_value






.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    2.590393731031688e-05



.. GENERATED FROM PYTHON SOURCE LINES 98-99

To get the best trial:

.. GENERATED FROM PYTHON SOURCE LINES 99-103

.. code-block:: Python


    study.best_trial






.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    FrozenTrial(number=71, state=TrialState.COMPLETE, values=[2.590393731031688e-05], datetime_start=datetime.datetime(2024, 3, 18, 12, 31, 22, 637586), datetime_complete=datetime.datetime(2024, 3, 18, 12, 31, 22, 641162), params={'x': 2.0050895910749604}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'x': FloatDistribution(high=10.0, log=False, low=-10.0, step=None)}, trial_id=71, value=None)



.. GENERATED FROM PYTHON SOURCE LINES 104-105

To get all trials:

.. GENERATED FROM PYTHON SOURCE LINES 105-110

.. code-block:: Python


    study.trials
    for trial in study.trials[:2]:  # Show first two trials
        print(trial)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    FrozenTrial(number=0, state=TrialState.COMPLETE, values=[48.274880960213935], datetime_start=datetime.datetime(2024, 3, 18, 12, 31, 22, 411970), datetime_complete=datetime.datetime(2024, 3, 18, 12, 31, 22, 412664), params={'x': -4.94801273460361}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'x': FloatDistribution(high=10.0, log=False, low=-10.0, step=None)}, trial_id=0, value=None)
    FrozenTrial(number=1, state=TrialState.COMPLETE, values=[114.34360385128335], datetime_start=datetime.datetime(2024, 3, 18, 12, 31, 22, 412928), datetime_complete=datetime.datetime(2024, 3, 18, 12, 31, 22, 413165), params={'x': -8.693156870227021}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'x': FloatDistribution(high=10.0, log=False, low=-10.0, step=None)}, trial_id=1, value=None)




.. GENERATED FROM PYTHON SOURCE LINES 111-112

To get the number of trials:

.. GENERATED FROM PYTHON SOURCE LINES 112-116

.. code-block:: Python


    len(study.trials)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    100



.. GENERATED FROM PYTHON SOURCE LINES 117-118

By executing :func:`~optuna.study.Study.optimize` again, we can continue the optimization.

.. GENERATED FROM PYTHON SOURCE LINES 118-122

.. code-block:: Python


    study.optimize(objective, n_trials=100)









.. GENERATED FROM PYTHON SOURCE LINES 123-124

To get the updated number of trials:

.. GENERATED FROM PYTHON SOURCE LINES 124-128

.. code-block:: Python


    len(study.trials)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    200



.. GENERATED FROM PYTHON SOURCE LINES 129-131

As the objective function is so easy that the last 100 trials don't improve the result.
However, we can check the result again:

.. GENERATED FROM PYTHON SOURCE LINES 131-134

.. code-block:: Python

    best_params = study.best_params
    found_x = best_params["x"]
    print("Found x: {}, (x - 2)^2: {}".format(found_x, (found_x - 2) ** 2))




.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Found x: 2.000903919424077, (x - 2)^2: 8.17070325223643e-07





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 0.759 seconds)


.. _sphx_glr_download_tutorial_10_key_features_001_first.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: 001_first.ipynb <001_first.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: 001_first.py <001_first.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
