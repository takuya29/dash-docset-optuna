
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "tutorial/10_key_features/001_first.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_tutorial_10_key_features_001_first.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_tutorial_10_key_features_001_first.py:


.. _first:

Lightweight, versatile, and platform agnostic architecture
==========================================================

Optuna is entirely written in Python and has few dependencies.
This means that we can quickly move to the real example once you get interested in Optuna.


Quadratic Function Example
--------------------------

Usually, Optuna is used to optimize hyperparameters, but as an example,
let's optimize a simple quadratic function: :math:`(x - 2)^2`.

.. GENERATED FROM PYTHON SOURCE LINES 20-21

First of all, import :mod:`optuna`.

.. GENERATED FROM PYTHON SOURCE LINES 21-25

.. code-block:: default


    import optuna









.. GENERATED FROM PYTHON SOURCE LINES 26-27

In optuna, conventionally functions to be optimized are named `objective`.

.. GENERATED FROM PYTHON SOURCE LINES 27-34

.. code-block:: default



    def objective(trial):
        x = trial.suggest_float("x", -10, 10)
        return (x - 2) ** 2









.. GENERATED FROM PYTHON SOURCE LINES 35-50

This function returns the value of :math:`(x - 2)^2`. Our goal is to find the value of ``x``
that minimizes the output of the ``objective`` function. This is the "optimization."
During the optimization, Optuna repeatedly calls and evaluates the objective function with
different values of ``x``.

A :class:`~optuna.trial.Trial` object corresponds to a single execution of the objective
function and is internally instantiated upon each invocation of the function.

The `suggest` APIs (for example, :func:`~optuna.trial.Trial.suggest_float`) are called
inside the objective function to obtain parameters for a trial.
:func:`~optuna.trial.Trial.suggest_float` selects parameters uniformly within the range
provided. In our example, from :math:`-10` to :math:`10`.

To start the optimization, we create a study object and pass the objective function to method
:func:`~optuna.study.Study.optimize` as follows.

.. GENERATED FROM PYTHON SOURCE LINES 50-55

.. code-block:: default


    study = optuna.create_study()
    study.optimize(objective, n_trials=100)









.. GENERATED FROM PYTHON SOURCE LINES 56-57

You can get the best parameter as follows.

.. GENERATED FROM PYTHON SOURCE LINES 57-62

.. code-block:: default


    best_params = study.best_params
    found_x = best_params["x"]
    print("Found x: {}, (x - 2)^2: {}".format(found_x, (found_x - 2) ** 2))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Found x: 2.0410355305645234, (x - 2)^2: 0.0016839147687119308




.. GENERATED FROM PYTHON SOURCE LINES 63-64

We can see that the ``x`` value found by Optuna is close to the optimal value of ``2``.

.. GENERATED FROM PYTHON SOURCE LINES 66-70

.. note::
    When used to search for hyperparameters in machine learning,
    usually the objective function would return the loss or accuracy
    of the model.

.. GENERATED FROM PYTHON SOURCE LINES 73-85

Study Object
------------

Let us clarify the terminology in Optuna as follows:

* **Trial**: A single call of the objective function
* **Study**: An optimization session, which is a set of trials
* **Parameter**: A variable whose value is to be optimized, such as ``x`` in the above example

In Optuna, we use the study object to manage optimization.
Method :func:`~optuna.study.create_study` returns a study object.
A study object has useful properties for analyzing the optimization outcome.

.. GENERATED FROM PYTHON SOURCE LINES 87-88

To get the dictionary of parameter name and parameter values:

.. GENERATED FROM PYTHON SOURCE LINES 88-92

.. code-block:: default



    study.best_params





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    {'x': 2.0410355305645234}



.. GENERATED FROM PYTHON SOURCE LINES 93-94

To get the best observed value of the objective function:

.. GENERATED FROM PYTHON SOURCE LINES 94-98

.. code-block:: default


    study.best_value






.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    0.0016839147687119308



.. GENERATED FROM PYTHON SOURCE LINES 99-100

To get the best trial:

.. GENERATED FROM PYTHON SOURCE LINES 100-104

.. code-block:: default


    study.best_trial






.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    FrozenTrial(number=26, state=TrialState.COMPLETE, values=[0.0016839147687119308], datetime_start=datetime.datetime(2023, 8, 7, 12, 34, 46, 406859), datetime_complete=datetime.datetime(2023, 8, 7, 12, 34, 46, 411138), params={'x': 2.0410355305645234}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'x': FloatDistribution(high=10.0, log=False, low=-10.0, step=None)}, trial_id=26, value=None)



.. GENERATED FROM PYTHON SOURCE LINES 105-106

To get all trials:

.. GENERATED FROM PYTHON SOURCE LINES 106-111

.. code-block:: default


    study.trials
    for trial in study.trials[:2]:  # Show first two trials
        print(trial)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    FrozenTrial(number=0, state=TrialState.COMPLETE, values=[14.320895770106132], datetime_start=datetime.datetime(2023, 8, 7, 12, 34, 46, 325908), datetime_complete=datetime.datetime(2023, 8, 7, 12, 34, 46, 326454), params={'x': 5.784295941137021}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'x': FloatDistribution(high=10.0, log=False, low=-10.0, step=None)}, trial_id=0, value=None)
    FrozenTrial(number=1, state=TrialState.COMPLETE, values=[59.59643269841976], datetime_start=datetime.datetime(2023, 8, 7, 12, 34, 46, 326806), datetime_complete=datetime.datetime(2023, 8, 7, 12, 34, 46, 327093), params={'x': 9.719872583043049}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'x': FloatDistribution(high=10.0, log=False, low=-10.0, step=None)}, trial_id=1, value=None)




.. GENERATED FROM PYTHON SOURCE LINES 112-113

To get the number of trials:

.. GENERATED FROM PYTHON SOURCE LINES 113-117

.. code-block:: default


    len(study.trials)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    100



.. GENERATED FROM PYTHON SOURCE LINES 118-119

By executing :func:`~optuna.study.Study.optimize` again, we can continue the optimization.

.. GENERATED FROM PYTHON SOURCE LINES 119-123

.. code-block:: default


    study.optimize(objective, n_trials=100)









.. GENERATED FROM PYTHON SOURCE LINES 124-125

To get the updated number of trials:

.. GENERATED FROM PYTHON SOURCE LINES 125-129

.. code-block:: default


    len(study.trials)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    200



.. GENERATED FROM PYTHON SOURCE LINES 130-132

As the objective function is so easy that the last 100 trials don't improve the result.
However, we can check the result again:

.. GENERATED FROM PYTHON SOURCE LINES 132-135

.. code-block:: default

    best_params = study.best_params
    found_x = best_params["x"]
    print("Found x: {}, (x - 2)^2: {}".format(found_x, (found_x - 2) ** 2))




.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Found x: 2.000293548599258, (x - 2)^2: 8.61707801264043e-08





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  1.025 seconds)


.. _sphx_glr_download_tutorial_10_key_features_001_first.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example




    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: 001_first.py <001_first.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: 001_first.ipynb <001_first.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
