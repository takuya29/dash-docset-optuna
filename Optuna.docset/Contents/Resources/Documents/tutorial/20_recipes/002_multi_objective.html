
<!DOCTYPE html>

<html class="writer-html5" lang="en">
<head>
<meta charset="utf-8"/><meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Multi-objective Optimization with Optuna — Optuna 4.2.1 documentation</title>
<link href="../../_static/pygments.css?v=03e43079" rel="stylesheet" type="text/css"/>
<link href="../../_static/css/theme.css?v=e59714d7" rel="stylesheet" type="text/css"/>
<link href="../../_static/copybutton.css?v=76b2166b" rel="stylesheet" type="text/css"/>
<link href="../../_static/sg_gallery.css?v=d2d258e8" rel="stylesheet" type="text/css"/>
<link href="../../_static/sg_gallery-binder.css?v=f4aeca0c" rel="stylesheet" type="text/css"/>
<link href="../../_static/sg_gallery-dataframe.css?v=2082cf3c" rel="stylesheet" type="text/css"/>
<link href="../../_static/sg_gallery-rendered-html.css?v=1277b6f3" rel="stylesheet" type="text/css"/>
<link href="../../_static/css/custom.css?v=d0d4e556" rel="stylesheet" type="text/css"/>
<link href="../../_static/favicon.ico" rel="shortcut icon"/>
<script src="../../_static/jquery.js?v=5d32c60e"></script>
<script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
<script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js?v=0f7e680c"></script>
<script src="../../_static/doctools.js?v=888ff710"></script>
<script src="../../_static/sphinx_highlight.js?v=4825356b"></script>
<script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
<script src="../../_static/copybutton.js?v=a56c686a"></script>
<script src="../../_static/js/theme.js"></script>
<link href="../../genindex.html" rel="index" title="Index"/>
<link href="../../search.html" rel="search" title="Search"/>
</head>
<body class="wy-body-for-nav">
<div class="navbar">
<div class="navbar ml-auto">
<ul class="navbar-nav">
<li>
<a class="header_link" href="https://optuna.org/#key_features">Key Features</a>
</li>
<li>
<a class="header_link" href="https://optuna.org/#code_examples">Code Examples</a>
</li>
<li>
<a class="header_link" href="https://optuna.org/#installation">Installation</a>
</li>
<li>
<a class="header_link" href="https://optuna.org/#dashboard">Dashboard</a>
</li>
<li>
<a class="header_link" href="https://optuna.org/#hub">OptunaHub</a>
</li>
<li>
<a class="header_link" href="https://optuna.org/#blog">Blog</a>
</li>
<li>
<a class="header_link" href="https://optuna.org/#video">Videos</a>
</li>
<li>
<a class="header_link" href="https://optuna.org/#paper">Paper</a>
</li>
</ul>
</div>
</div>
<div class="wy-grid-for-nav">
<nav class="wy-nav-side" data-toggle="wy-nav-shift">
<div class="wy-side-scroll">
<div class="wy-side-nav-search">
<a href="../../index.html">
<img alt="Logo" class="logo" src="../../_static/optuna-logo.png"/>
</a>
<div role="search">
<form action="../../search.html" class="wy-form" id="rtd-search-form" method="get">
<input aria-label="Search docs" name="q" placeholder="Search docs" type="text"/>
<input name="check_keywords" type="hidden" value="yes"/>
<input name="area" type="hidden" value="default"/>
</form>
</div>
</div><div aria-label="Navigation menu" class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation">
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../index.html">Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../reference/index.html">API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq.html">FAQ</a></li>
</ul>
</div>
</div>
</nav>
<section class="wy-nav-content-wrap" data-toggle="wy-nav-shift"><nav aria-label="Mobile navigation menu" class="wy-nav-top">
<i class="fa fa-bars" data-toggle="wy-nav-top"></i>
<a href="../../index.html">Optuna</a>
</nav>
<div class="wy-nav-content">
<div class="rst-content">
<!-- This file is necessary to remove "Edit on Github" button from readthedocs by following https://docs.readthedocs.io/en/stable/guides/remove-edit-buttons.html#remove-links-from-top-right-corner --><div aria-label="Page navigation" role="navigation">
<ul class="wy-breadcrumbs">
<li><a aria-label="Home" class="icon icon-home" href="../../index.html"></a></li>
<li class="breadcrumb-item active">Multi-objective Optimization with Optuna</li>
</ul>
<hr/>
</div>
<div class="document" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
<div itemprop="articleBody">
<div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#sphx-glr-download-tutorial-20-recipes-002-multi-objective-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code.</p>
</div>
<section class="sphx-glr-example-title" id="multi-objective-optimization-with-optuna">
<a class="dashAnchor" name="//apple_ref/cpp/Section/Multi-objective Optimization with Optuna"></a><span id="multi-objective"></span><a class="dashAnchor" name="//apple_ref/cpp/Section/Multi-objective Optimization with Optuna"></a><span id="sphx-glr-tutorial-20-recipes-002-multi-objective-py"></span><h1>Multi-objective Optimization with Optuna<a class="headerlink" href="#multi-objective-optimization-with-optuna" title="Permalink to this heading"></a></h1>
<p>This tutorial showcases Optuna’s multi-objective optimization feature by
optimizing the validation accuracy of Fashion MNIST dataset and the FLOPS of the model implemented in PyTorch.</p>
<p>We use <a class="reference external" href="https://github.com/facebookresearch/fvcore">fvcore</a> to measure FLOPS.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torchvision</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">fvcore.nn</span><span class="w"> </span><span class="kn">import</span> <span class="n">FlopCountAnalysis</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">optuna</span>


<a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device"><span class="n">DEVICE</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device"><span class="n">torch</span><span class="o">.</span><span class="n">device</span></a><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span> <span class="k">if</span> <a class="sphx-glr-backref-module-torch-cuda sphx-glr-backref-type-py-function" href="https://pytorch.org/docs/stable/generated/torch.cuda.is_available.html#torch.cuda.is_available" title="torch.cuda.is_available"><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span></a><span class="p">()</span> <span class="k">else</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device"><span class="n">torch</span><span class="o">.</span><span class="n">device</span></a><span class="p">(</span><span class="s2">"cpu"</span><span class="p">)</span>
<a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str"><span class="n">DIR</span></a> <span class="o">=</span> <span class="s2">".."</span>
<a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.python.org/3/library/functions.html#int" title="builtins.int"><span class="n">BATCHSIZE</span></a> <span class="o">=</span> <span class="mi">128</span>
<a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.python.org/3/library/functions.html#int" title="builtins.int"><span class="n">N_TRAIN_EXAMPLES</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.python.org/3/library/functions.html#int" title="builtins.int"><span class="n">BATCHSIZE</span></a> <span class="o">*</span> <span class="mi">30</span>
<a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.python.org/3/library/functions.html#int" title="builtins.int"><span class="n">N_VALID_EXAMPLES</span></a> <span class="o">=</span> <a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.python.org/3/library/functions.html#int" title="builtins.int"><span class="n">BATCHSIZE</span></a> <span class="o">*</span> <span class="mi">10</span>


<span class="k">def</span><span class="w"> </span><span class="nf">define_model</span><span class="p">(</span><span class="n">trial</span><span class="p">):</span>
    <span class="n">n_layers</span> <span class="o">=</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="s2">"n_layers"</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">in_features</span> <span class="o">=</span> <span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layers</span><span class="p">):</span>
        <span class="n">out_features</span> <span class="o">=</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="s2">"n_units_l</span><span class="si">{}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">),</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear"><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">))</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU" title="torch.nn.ReLU"><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span></a><span class="p">())</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_float</span><span class="p">(</span><span class="s2">"dropout_</span><span class="si">{}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">),</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout" title="torch.nn.Dropout"><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span></a><span class="p">(</span><span class="n">p</span><span class="p">))</span>

        <span class="n">in_features</span> <span class="o">=</span> <span class="n">out_features</span>

    <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear"><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html#torch.nn.LogSoftmax" title="torch.nn.LogSoftmax"><span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span></a><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

    <span class="k">return</span> <a class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class" href="https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential" title="torch.nn.Sequential"><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span></a><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>


<span class="c1"># Defines training and evaluation.</span>
<span class="k">def</span><span class="w"> </span><span class="nf">train_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
        <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device"><span class="n">DEVICE</span></a><span class="p">),</span> <span class="n">target</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device"><span class="n">DEVICE</span></a><span class="p">)</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <a class="sphx-glr-backref-module-torch-nn-functional sphx-glr-backref-type-py-function" href="https://pytorch.org/docs/stable/generated/torch.nn.functional.nll_loss.html#torch.nn.functional.nll_loss" title="torch.nn.functional.nll_loss"><span class="n">F</span><span class="o">.</span><span class="n">nll_loss</span></a><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="n">target</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>


<span class="k">def</span><span class="w"> </span><span class="nf">eval_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">valid_loader</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">with</span> <a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class" href="https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad" title="torch.no_grad"><span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span></a><span class="p">():</span>
        <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">valid_loader</span><span class="p">):</span>
            <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device"><span class="n">DEVICE</span></a><span class="p">),</span> <span class="n">target</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device"><span class="n">DEVICE</span></a><span class="p">)</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">correct</span> <span class="o">+=</span> <span class="n">pred</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">view_as</span><span class="p">(</span><span class="n">pred</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">correct</span> <span class="o">/</span> <a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.python.org/3/library/functions.html#int" title="builtins.int"><span class="n">N_VALID_EXAMPLES</span></a>

    <span class="n">flops</span> <span class="o">=</span> <span class="n">FlopCountAnalysis</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function" href="https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device"><span class="n">DEVICE</span></a><span class="p">),))</span><span class="o">.</span><span class="n">total</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">flops</span><span class="p">,</span> <span class="n">accuracy</span>
</pre></div>
</div>
<p>Define multi-objective objective function.
Objectives are FLOPS and accuracy.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">objective</span><span class="p">(</span><span class="n">trial</span><span class="p">):</span>
    <span class="n">train_dataset</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset" title="torch.utils.data.Dataset"><span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">FashionMNIST</span></a><span class="p">(</span>
        <a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str"><span class="n">DIR</span></a><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">torchvision</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">()</span>
    <span class="p">)</span>
    <span class="n">train_loader</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader"><span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span></a><span class="p">(</span>
        <a class="sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset" title="torch.utils.data.Subset"><span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Subset</span></a><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.python.org/3/library/functions.html#int" title="builtins.int"><span class="n">N_TRAIN_EXAMPLES</span></a><span class="p">))),</span>
        <span class="n">batch_size</span><span class="o">=</span><a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.python.org/3/library/functions.html#int" title="builtins.int"><span class="n">BATCHSIZE</span></a><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">val_dataset</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset" title="torch.utils.data.Dataset"><span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">FashionMNIST</span></a><span class="p">(</span>
        <a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str"><span class="n">DIR</span></a><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">torchvision</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">()</span>
    <span class="p">)</span>
    <span class="n">val_loader</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader"><span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span></a><span class="p">(</span>
        <a class="sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset" title="torch.utils.data.Subset"><span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Subset</span></a><span class="p">(</span><span class="n">val_dataset</span><span class="p">,</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.python.org/3/library/functions.html#int" title="builtins.int"><span class="n">N_VALID_EXAMPLES</span></a><span class="p">))),</span>
        <span class="n">batch_size</span><span class="o">=</span><a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.python.org/3/library/functions.html#int" title="builtins.int"><span class="n">BATCHSIZE</span></a><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">define_model</span><span class="p">(</span><span class="n">trial</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><a class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device"><span class="n">DEVICE</span></a><span class="p">)</span>

    <span class="n">optimizer</span> <span class="o">=</span> <a class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class" href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam" title="torch.optim.Adam"><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span></a><span class="p">(</span>
        <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_float</span><span class="p">(</span><span class="s2">"lr"</span><span class="p">,</span> <span class="mf">1e-5</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">,</span> <span class="n">log</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="n">train_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">)</span>
    <span class="n">flops</span><span class="p">,</span> <span class="n">accuracy</span> <span class="o">=</span> <span class="n">eval_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">flops</span><span class="p">,</span> <span class="n">accuracy</span>
</pre></div>
</div>
<section id="run-multi-objective-optimization">
<h2>Run multi-objective optimization<a class="headerlink" href="#run-multi-objective-optimization" title="Permalink to this heading"></a></h2>
<p>If your optimization problem is multi-objective,
Optuna assumes that you will specify the optimization direction for each objective.
Specifically, in this example, we want to minimize the FLOPS (we want a faster model)
and maximize the accuracy. So we set <code class="docutils literal notranslate"><span class="pre">directions</span></code> to <code class="docutils literal notranslate"><span class="pre">["minimize",</span> <span class="pre">"maximize"]</span></code>.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">study</span> <span class="o">=</span> <span class="n">optuna</span><span class="o">.</span><span class="n">create_study</span><span class="p">(</span><span class="n">directions</span><span class="o">=</span><span class="p">[</span><span class="s2">"minimize"</span><span class="p">,</span> <span class="s2">"maximize"</span><span class="p">])</span>
<span class="n">study</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">objective</span><span class="p">,</span> <span class="n">n_trials</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Number of finished trials: "</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">study</span><span class="o">.</span><span class="n">trials</span><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Unsupported operator aten::log_softmax encountered 1 time(s)
Unsupported operator aten::log_softmax encountered 1 time(s)
Unsupported operator aten::log_softmax encountered 1 time(s)
Unsupported operator aten::log_softmax encountered 1 time(s)
Unsupported operator aten::log_softmax encountered 1 time(s)
Unsupported operator aten::log_softmax encountered 1 time(s)
Unsupported operator aten::log_softmax encountered 1 time(s)
Unsupported operator aten::log_softmax encountered 1 time(s)
Unsupported operator aten::log_softmax encountered 1 time(s)
Unsupported operator aten::log_softmax encountered 1 time(s)
Unsupported operator aten::log_softmax encountered 1 time(s)
Unsupported operator aten::log_softmax encountered 1 time(s)
Unsupported operator aten::log_softmax encountered 1 time(s)
Unsupported operator aten::log_softmax encountered 1 time(s)
Unsupported operator aten::log_softmax encountered 1 time(s)
Unsupported operator aten::log_softmax encountered 1 time(s)
Unsupported operator aten::log_softmax encountered 1 time(s)
Unsupported operator aten::log_softmax encountered 1 time(s)
Unsupported operator aten::log_softmax encountered 1 time(s)
Unsupported operator aten::log_softmax encountered 1 time(s)
Unsupported operator aten::log_softmax encountered 1 time(s)
Unsupported operator aten::log_softmax encountered 1 time(s)
Unsupported operator aten::log_softmax encountered 1 time(s)
Unsupported operator aten::log_softmax encountered 1 time(s)
Unsupported operator aten::log_softmax encountered 1 time(s)
Unsupported operator aten::log_softmax encountered 1 time(s)
Unsupported operator aten::log_softmax encountered 1 time(s)
Unsupported operator aten::log_softmax encountered 1 time(s)
Unsupported operator aten::log_softmax encountered 1 time(s)
Unsupported operator aten::log_softmax encountered 1 time(s)
Number of finished trials:  30
</pre></div>
</div>
<p>Note that the following sections requires the installation of <a class="reference external" href="https://plotly.com/python">Plotly</a> for visualization
and <a class="reference external" href="https://scikit-learn.org/stable">scikit-learn</a> for hyperparameter importance calculation:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>pip<span class="w"> </span>install<span class="w"> </span>plotly
<span class="gp">$ </span>pip<span class="w"> </span>install<span class="w"> </span>scikit-learn
<span class="gp">$ </span>pip<span class="w"> </span>install<span class="w"> </span>nbformat<span class="w">  </span><span class="c1"># Required if you are running this tutorial in Jupyter Notebook.</span>
</pre></div>
</div>
<p>Check trials on Pareto front visually.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">optuna</span><span class="o">.</span><span class="n">visualization</span><span class="o">.</span><span class="n">plot_pareto_front</span><span class="p">(</span><span class="n">study</span><span class="p">,</span> <span class="n">target_names</span><span class="o">=</span><span class="p">[</span><span class="s2">"FLOPS"</span><span class="p">,</span> <span class="s2">"accuracy"</span><span class="p">])</span>
</pre></div>
</div>
<div class="output_subarea output_html rendered_html output_result">
<div> <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
<script charset="utf-8" src="https://cdn.plot.ly/plotly-3.0.1.min.js"></script> <div class="plotly-graph-div" id="d70d3d52-cf7b-446a-995c-51e5dbace814" style="height:100%; width:100%;"></div> <script type="text/javascript">                window.PLOTLYENV=window.PLOTLYENV || {};                                if (document.getElementById("d70d3d52-cf7b-446a-995c-51e5dbace814")) {                    Plotly.newPlot(                        "d70d3d52-cf7b-446a-995c-51e5dbace814",                        [{"hovertemplate":"%{text}\u003cextra\u003eTrial\u003c\u002fextra\u003e","marker":{"color":[0,1,2,3,5,6,7,8,10,11,12,13,14,15,17,19,21,22,23,24,25,26,27,28,29],"colorbar":{"title":{"text":"Trial"}},"colorscale":[[0.0,"rgb(247,251,255)"],[0.125,"rgb(222,235,247)"],[0.25,"rgb(198,219,239)"],[0.375,"rgb(158,202,225)"],[0.5,"rgb(107,174,214)"],[0.625,"rgb(66,146,198)"],[0.75,"rgb(33,113,181)"],[0.875,"rgb(8,81,156)"],[1.0,"rgb(8,48,107)"]],"line":{"color":"Grey","width":0.5}},"mode":"markers","showlegend":false,"text":["{\u003cbr\u003e  \"number\": 0,\u003cbr\u003e  \"values\": [\u003cbr\u003e    108600.0,\u003cbr\u003e    0.68203125\u003cbr\u003e  ],\u003cbr\u003e  \"params\": {\u003cbr\u003e    \"n_layers\": 3,\u003cbr\u003e    \"n_units_l0\": 125,\u003cbr\u003e    \"dropout_0\": 0.30564657957785074,\u003cbr\u003e    \"n_units_l1\": 40,\u003cbr\u003e    \"dropout_1\": 0.22106118824041288,\u003cbr\u003e    \"n_units_l2\": 112,\u003cbr\u003e    \"dropout_2\": 0.42590928320101207,\u003cbr\u003e    \"lr\": 0.019430722813220287\u003cbr\u003e  }\u003cbr\u003e}","{\u003cbr\u003e  \"number\": 1,\u003cbr\u003e  \"values\": [\u003cbr\u003e    59276.0,\u003cbr\u003e    0.621875\u003cbr\u003e  ],\u003cbr\u003e  \"params\": {\u003cbr\u003e    \"n_layers\": 2,\u003cbr\u003e    \"n_units_l0\": 74,\u003cbr\u003e    \"dropout_0\": 0.31268986112855873,\u003cbr\u003e    \"n_units_l1\": 15,\u003cbr\u003e    \"dropout_1\": 0.38251852715723783,\u003cbr\u003e    \"lr\": 0.00010476229109021675\u003cbr\u003e  }\u003cbr\u003e}","{\u003cbr\u003e  \"number\": 2,\u003cbr\u003e  \"values\": [\u003cbr\u003e    59812.0,\u003cbr\u003e    0.7296875\u003cbr\u003e  ],\u003cbr\u003e  \"params\": {\u003cbr\u003e    \"n_layers\": 3,\u003cbr\u003e    \"n_units_l0\": 70,\u003cbr\u003e    \"dropout_0\": 0.3026490563040735,\u003cbr\u003e    \"n_units_l1\": 54,\u003cbr\u003e    \"dropout_1\": 0.4848157187989814,\u003cbr\u003e    \"n_units_l2\": 18,\u003cbr\u003e    \"dropout_2\": 0.3872234316030744,\u003cbr\u003e    \"lr\": 0.0016624646435359934\u003cbr\u003e  }\u003cbr\u003e}","{\u003cbr\u003e  \"number\": 3,\u003cbr\u003e  \"values\": [\u003cbr\u003e    28896.0,\u003cbr\u003e    0.81484375\u003cbr\u003e  ],\u003cbr\u003e  \"params\": {\u003cbr\u003e    \"n_layers\": 2,\u003cbr\u003e    \"n_units_l0\": 31,\u003cbr\u003e    \"dropout_0\": 0.23135467852081226,\u003cbr\u003e    \"n_units_l1\": 112,\u003cbr\u003e    \"dropout_1\": 0.4132207177602375,\u003cbr\u003e    \"lr\": 0.00548921802339078\u003cbr\u003e  }\u003cbr\u003e}","{\u003cbr\u003e  \"number\": 5,\u003cbr\u003e  \"values\": [\u003cbr\u003e    21438.0,\u003cbr\u003e    0.55703125\u003cbr\u003e  ],\u003cbr\u003e  \"params\": {\u003cbr\u003e    \"n_layers\": 1,\u003cbr\u003e    \"n_units_l0\": 27,\u003cbr\u003e    \"dropout_0\": 0.48177662125984805,\u003cbr\u003e    \"lr\": 6.451570037306254e-05\u003cbr\u003e  }\u003cbr\u003e}","{\u003cbr\u003e  \"number\": 6,\u003cbr\u003e  \"values\": [\u003cbr\u003e    57410.0,\u003cbr\u003e    0.44609375\u003cbr\u003e  ],\u003cbr\u003e  \"params\": {\u003cbr\u003e    \"n_layers\": 2,\u003cbr\u003e    \"n_units_l0\": 65,\u003cbr\u003e    \"dropout_0\": 0.3107110317106019,\u003cbr\u003e    \"n_units_l1\": 86,\u003cbr\u003e    \"dropout_1\": 0.44131647743080304,\u003cbr\u003e    \"lr\": 2.851649174865897e-05\u003cbr\u003e  }\u003cbr\u003e}","{\u003cbr\u003e  \"number\": 7,\u003cbr\u003e  \"values\": [\u003cbr\u003e    66608.0,\u003cbr\u003e    0.7234375\u003cbr\u003e  ],\u003cbr\u003e  \"params\": {\u003cbr\u003e    \"n_layers\": 2,\u003cbr\u003e    \"n_units_l0\": 78,\u003cbr\u003e    \"dropout_0\": 0.49536592333851054,\u003cbr\u003e    \"n_units_l1\": 62,\u003cbr\u003e    \"dropout_1\": 0.20044020206562416,\u003cbr\u003e    \"lr\": 0.00026676251216729325\u003cbr\u003e  }\u003cbr\u003e}","{\u003cbr\u003e  \"number\": 8,\u003cbr\u003e  \"values\": [\u003cbr\u003e    59922.0,\u003cbr\u003e    0.24296875\u003cbr\u003e  ],\u003cbr\u003e  \"params\": {\u003cbr\u003e    \"n_layers\": 3,\u003cbr\u003e    \"n_units_l0\": 74,\u003cbr\u003e    \"dropout_0\": 0.42747116557239306,\u003cbr\u003e    \"n_units_l1\": 8,\u003cbr\u003e    \"dropout_1\": 0.4523876868314261,\u003cbr\u003e    \"n_units_l2\": 73,\u003cbr\u003e    \"dropout_2\": 0.37897110931481326,\u003cbr\u003e    \"lr\": 3.902234232487997e-05\u003cbr\u003e  }\u003cbr\u003e}","{\u003cbr\u003e  \"number\": 10,\u003cbr\u003e  \"values\": [\u003cbr\u003e    82576.0,\u003cbr\u003e    0.58671875\u003cbr\u003e  ],\u003cbr\u003e  \"params\": {\u003cbr\u003e    \"n_layers\": 1,\u003cbr\u003e    \"n_units_l0\": 104,\u003cbr\u003e    \"dropout_0\": 0.344514498292647,\u003cbr\u003e    \"lr\": 1.6238188864364444e-05\u003cbr\u003e  }\u003cbr\u003e}","{\u003cbr\u003e  \"number\": 11,\u003cbr\u003e  \"values\": [\u003cbr\u003e    64122.0,\u003cbr\u003e    0.490625\u003cbr\u003e  ],\u003cbr\u003e  \"params\": {\u003cbr\u003e    \"n_layers\": 3,\u003cbr\u003e    \"n_units_l0\": 79,\u003cbr\u003e    \"dropout_0\": 0.48193620181029345,\u003cbr\u003e    \"n_units_l1\": 14,\u003cbr\u003e    \"dropout_1\": 0.32108534040742603,\u003cbr\u003e    \"n_units_l2\": 45,\u003cbr\u003e    \"dropout_2\": 0.3399284863705113,\u003cbr\u003e    \"lr\": 0.00016126423006658955\u003cbr\u003e  }\u003cbr\u003e}","{\u003cbr\u003e  \"number\": 12,\u003cbr\u003e  \"values\": [\u003cbr\u003e    16674.0,\u003cbr\u003e    0.453125\u003cbr\u003e  ],\u003cbr\u003e  \"params\": {\u003cbr\u003e    \"n_layers\": 1,\u003cbr\u003e    \"n_units_l0\": 21,\u003cbr\u003e    \"dropout_0\": 0.2901110096331695,\u003cbr\u003e    \"lr\": 0.07065718426363007\u003cbr\u003e  }\u003cbr\u003e}","{\u003cbr\u003e  \"number\": 13,\u003cbr\u003e  \"values\": [\u003cbr\u003e    106829.0,\u003cbr\u003e    0.56953125\u003cbr\u003e  ],\u003cbr\u003e  \"params\": {\u003cbr\u003e    \"n_layers\": 2,\u003cbr\u003e    \"n_units_l0\": 127,\u003cbr\u003e    \"dropout_0\": 0.31366056566417677,\u003cbr\u003e    \"n_units_l1\": 53,\u003cbr\u003e    \"dropout_1\": 0.2858972583184024,\u003cbr\u003e    \"lr\": 3.689732917492064e-05\u003cbr\u003e  }\u003cbr\u003e}","{\u003cbr\u003e  \"number\": 14,\u003cbr\u003e  \"values\": [\u003cbr\u003e    73815.0,\u003cbr\u003e    0.1046875\u003cbr\u003e  ],\u003cbr\u003e  \"params\": {\u003cbr\u003e    \"n_layers\": 3,\u003cbr\u003e    \"n_units_l0\": 79,\u003cbr\u003e    \"dropout_0\": 0.2594497263001819,\u003cbr\u003e    \"n_units_l1\": 113,\u003cbr\u003e    \"dropout_1\": 0.4463785981933836,\u003cbr\u003e    \"n_units_l2\": 24,\u003cbr\u003e    \"dropout_2\": 0.45782221682192104,\u003cbr\u003e    \"lr\": 1.1727547439766146e-05\u003cbr\u003e  }\u003cbr\u003e}","{\u003cbr\u003e  \"number\": 15,\u003cbr\u003e  \"values\": [\u003cbr\u003e    73842.0,\u003cbr\u003e    0.8\u003cbr\u003e  ],\u003cbr\u003e  \"params\": {\u003cbr\u003e    \"n_layers\": 1,\u003cbr\u003e    \"n_units_l0\": 93,\u003cbr\u003e    \"dropout_0\": 0.42870532199370653,\u003cbr\u003e    \"lr\": 0.013433552374125536\u003cbr\u003e  }\u003cbr\u003e}","{\u003cbr\u003e  \"number\": 17,\u003cbr\u003e  \"values\": [\u003cbr\u003e    47034.0,\u003cbr\u003e    0.77734375\u003cbr\u003e  ],\u003cbr\u003e  \"params\": {\u003cbr\u003e    \"n_layers\": 3,\u003cbr\u003e    \"n_units_l0\": 39,\u003cbr\u003e    \"dropout_0\": 0.30009755518801373,\u003cbr\u003e    \"n_units_l1\": 98,\u003cbr\u003e    \"dropout_1\": 0.4775884256474741,\u003cbr\u003e    \"n_units_l2\": 117,\u003cbr\u003e    \"dropout_2\": 0.31660093462069017,\u003cbr\u003e    \"lr\": 0.0023171295429968842\u003cbr\u003e  }\u003cbr\u003e}","{\u003cbr\u003e  \"number\": 19,\u003cbr\u003e  \"values\": [\u003cbr\u003e    99890.0,\u003cbr\u003e    0.78359375\u003cbr\u003e  ],\u003cbr\u003e  \"params\": {\u003cbr\u003e    \"n_layers\": 2,\u003cbr\u003e    \"n_units_l0\": 125,\u003cbr\u003e    \"dropout_0\": 0.310341205653076,\u003cbr\u003e    \"n_units_l1\": 14,\u003cbr\u003e    \"dropout_1\": 0.49664310022101754,\u003cbr\u003e    \"lr\": 0.00623123666092069\u003cbr\u003e  }\u003cbr\u003e}","{\u003cbr\u003e  \"number\": 21,\u003cbr\u003e  \"values\": [\u003cbr\u003e    102010.0,\u003cbr\u003e    0.81875\u003cbr\u003e  ],\u003cbr\u003e  \"params\": {\u003cbr\u003e    \"n_layers\": 2,\u003cbr\u003e    \"n_units_l0\": 120,\u003cbr\u003e    \"dropout_0\": 0.3586473277540202,\u003cbr\u003e    \"n_units_l1\": 61,\u003cbr\u003e    \"dropout_1\": 0.3929177371615624,\u003cbr\u003e    \"lr\": 0.00183498949866818\u003cbr\u003e  }\u003cbr\u003e}","{\u003cbr\u003e  \"number\": 22,\u003cbr\u003e  \"values\": [\u003cbr\u003e    72960.0,\u003cbr\u003e    0.29765625\u003cbr\u003e  ],\u003cbr\u003e  \"params\": {\u003cbr\u003e    \"n_layers\": 2,\u003cbr\u003e    \"n_units_l0\": 90,\u003cbr\u003e    \"dropout_0\": 0.2001158579194651,\u003cbr\u003e    \"n_units_l1\": 24,\u003cbr\u003e    \"dropout_1\": 0.21772756525386688,\u003cbr\u003e    \"lr\": 2.4847180886235587e-05\u003cbr\u003e  }\u003cbr\u003e}","{\u003cbr\u003e  \"number\": 23,\u003cbr\u003e  \"values\": [\u003cbr\u003e    15632.0,\u003cbr\u003e    0.1234375\u003cbr\u003e  ],\u003cbr\u003e  \"params\": {\u003cbr\u003e    \"n_layers\": 3,\u003cbr\u003e    \"n_units_l0\": 16,\u003cbr\u003e    \"dropout_0\": 0.26626701377838424,\u003cbr\u003e    \"n_units_l1\": 46,\u003cbr\u003e    \"dropout_1\": 0.42823887448470843,\u003cbr\u003e    \"n_units_l2\": 42,\u003cbr\u003e    \"dropout_2\": 0.3945614687579563,\u003cbr\u003e    \"lr\": 1.8668564812615323e-05\u003cbr\u003e  }\u003cbr\u003e}","{\u003cbr\u003e  \"number\": 24,\u003cbr\u003e  \"values\": [\u003cbr\u003e    6938.0,\u003cbr\u003e    0.41328125\u003cbr\u003e  ],\u003cbr\u003e  \"params\": {\u003cbr\u003e    \"n_layers\": 2,\u003cbr\u003e    \"n_units_l0\": 8,\u003cbr\u003e    \"dropout_0\": 0.3223283346440807,\u003cbr\u003e    \"n_units_l1\": 37,\u003cbr\u003e    \"dropout_1\": 0.2617977014969727,\u003cbr\u003e    \"lr\": 8.040424755439464e-05\u003cbr\u003e  }\u003cbr\u003e}","{\u003cbr\u003e  \"number\": 25,\u003cbr\u003e  \"values\": [\u003cbr\u003e    73048.0,\u003cbr\u003e    0.81796875\u003cbr\u003e  ],\u003cbr\u003e  \"params\": {\u003cbr\u003e    \"n_layers\": 1,\u003cbr\u003e    \"n_units_l0\": 92,\u003cbr\u003e    \"dropout_0\": 0.42094514048043474,\u003cbr\u003e    \"lr\": 0.0013033725594266638\u003cbr\u003e  }\u003cbr\u003e}","{\u003cbr\u003e  \"number\": 26,\u003cbr\u003e  \"values\": [\u003cbr\u003e    104930.0,\u003cbr\u003e    0.8203125\u003cbr\u003e  ],\u003cbr\u003e  \"params\": {\u003cbr\u003e    \"n_layers\": 2,\u003cbr\u003e    \"n_units_l0\": 116,\u003cbr\u003e    \"dropout_0\": 0.41400878973064204,\u003cbr\u003e    \"n_units_l1\": 111,\u003cbr\u003e    \"dropout_1\": 0.36116525643325736,\u003cbr\u003e    \"lr\": 0.003399484147688439\u003cbr\u003e  }\u003cbr\u003e}","{\u003cbr\u003e  \"number\": 27,\u003cbr\u003e  \"values\": [\u003cbr\u003e    87610.0,\u003cbr\u003e    0.81953125\u003cbr\u003e  ],\u003cbr\u003e  \"params\": {\u003cbr\u003e    \"n_layers\": 2,\u003cbr\u003e    \"n_units_l0\": 105,\u003cbr\u003e    \"dropout_0\": 0.41518839897843507,\u003cbr\u003e    \"n_units_l1\": 46,\u003cbr\u003e    \"dropout_1\": 0.42905978238976145,\u003cbr\u003e    \"lr\": 0.008407367708741665\u003cbr\u003e  }\u003cbr\u003e}","{\u003cbr\u003e  \"number\": 28,\u003cbr\u003e  \"values\": [\u003cbr\u003e    39641.0,\u003cbr\u003e    0.7328125\u003cbr\u003e  ],\u003cbr\u003e  \"params\": {\u003cbr\u003e    \"n_layers\": 2,\u003cbr\u003e    \"n_units_l0\": 47,\u003cbr\u003e    \"dropout_0\": 0.38960217029962696,\u003cbr\u003e    \"n_units_l1\": 49,\u003cbr\u003e    \"dropout_1\": 0.20881705916716742,\u003cbr\u003e    \"lr\": 0.0004930714716242038\u003cbr\u003e  }\u003cbr\u003e}","{\u003cbr\u003e  \"number\": 29,\u003cbr\u003e  \"values\": [\u003cbr\u003e    35552.0,\u003cbr\u003e    0.57578125\u003cbr\u003e  ],\u003cbr\u003e  \"params\": {\u003cbr\u003e    \"n_layers\": 2,\u003cbr\u003e    \"n_units_l0\": 38,\u003cbr\u003e    \"dropout_0\": 0.38365699463615166,\u003cbr\u003e    \"n_units_l1\": 120,\u003cbr\u003e    \"dropout_1\": 0.29358765708741813,\u003cbr\u003e    \"lr\": 5.055166151775184e-05\u003cbr\u003e  }\u003cbr\u003e}"],"x":[108600.0,59276.0,59812.0,28896.0,21438.0,57410.0,66608.0,59922.0,82576.0,64122.0,16674.0,106829.0,73815.0,73842.0,47034.0,99890.0,102010.0,72960.0,15632.0,6938.0,73048.0,104930.0,87610.0,39641.0,35552.0],"y":[0.68203125,0.621875,0.7296875,0.81484375,0.55703125,0.44609375,0.7234375,0.24296875,0.58671875,0.490625,0.453125,0.56953125,0.1046875,0.8,0.77734375,0.78359375,0.81875,0.29765625,0.1234375,0.41328125,0.81796875,0.8203125,0.81953125,0.7328125,0.57578125],"type":"scatter"},{"hovertemplate":"%{text}\u003cextra\u003eBest Trial\u003c\u002fextra\u003e","marker":{"color":[4,9,16,18,20],"colorbar":{"title":{"text":"Best Trial"},"x":1.1,"xpad":40},"colorscale":[[0.0,"rgb(255,245,240)"],[0.125,"rgb(254,224,210)"],[0.25,"rgb(252,187,161)"],[0.375,"rgb(252,146,114)"],[0.5,"rgb(251,106,74)"],[0.625,"rgb(239,59,44)"],[0.75,"rgb(203,24,29)"],[0.875,"rgb(165,15,21)"],[1.0,"rgb(103,0,13)"]],"line":{"color":"Grey","width":0.5}},"mode":"markers","showlegend":false,"text":["{\u003cbr\u003e  \"number\": 4,\u003cbr\u003e  \"values\": [\u003cbr\u003e    5248.0,\u003cbr\u003e    0.515625\u003cbr\u003e  ],\u003cbr\u003e  \"params\": {\u003cbr\u003e    \"n_layers\": 2,\u003cbr\u003e    \"n_units_l0\": 6,\u003cbr\u003e    \"dropout_0\": 0.38178117031172765,\u003cbr\u003e    \"n_units_l1\": 34,\u003cbr\u003e    \"dropout_1\": 0.3320433716345862,\u003cbr\u003e    \"lr\": 0.00034852212252765917\u003cbr\u003e  }\u003cbr\u003e}","{\u003cbr\u003e  \"number\": 9,\u003cbr\u003e  \"values\": [\u003cbr\u003e    4764.0,\u003cbr\u003e    0.1015625\u003cbr\u003e  ],\u003cbr\u003e  \"params\": {\u003cbr\u003e    \"n_layers\": 1,\u003cbr\u003e    \"n_units_l0\": 6,\u003cbr\u003e    \"dropout_0\": 0.3676222090862027,\u003cbr\u003e    \"lr\": 0.01760150882878357\u003cbr\u003e  }\u003cbr\u003e}","{\u003cbr\u003e  \"number\": 16,\u003cbr\u003e  \"values\": [\u003cbr\u003e    7146.0,\u003cbr\u003e    0.73671875\u003cbr\u003e  ],\u003cbr\u003e  \"params\": {\u003cbr\u003e    \"n_layers\": 1,\u003cbr\u003e    \"n_units_l0\": 9,\u003cbr\u003e    \"dropout_0\": 0.427020514751434,\u003cbr\u003e    \"lr\": 0.011071379396353179\u003cbr\u003e  }\u003cbr\u003e}","{\u003cbr\u003e  \"number\": 18,\u003cbr\u003e  \"values\": [\u003cbr\u003e    19850.0,\u003cbr\u003e    0.8265625\u003cbr\u003e  ],\u003cbr\u003e  \"params\": {\u003cbr\u003e    \"n_layers\": 1,\u003cbr\u003e    \"n_units_l0\": 25,\u003cbr\u003e    \"dropout_0\": 0.20746282024429416,\u003cbr\u003e    \"lr\": 0.0036824670106646337\u003cbr\u003e  }\u003cbr\u003e}","{\u003cbr\u003e  \"number\": 20,\u003cbr\u003e  \"values\": [\u003cbr\u003e    50816.0,\u003cbr\u003e    0.83203125\u003cbr\u003e  ],\u003cbr\u003e  \"params\": {\u003cbr\u003e    \"n_layers\": 1,\u003cbr\u003e    \"n_units_l0\": 64,\u003cbr\u003e    \"dropout_0\": 0.4427151058715231,\u003cbr\u003e    \"lr\": 0.003991493208779521\u003cbr\u003e  }\u003cbr\u003e}"],"x":[5248.0,4764.0,7146.0,19850.0,50816.0],"y":[0.515625,0.1015625,0.73671875,0.8265625,0.83203125],"type":"scatter"}],                        {"title":{"text":"Pareto-front Plot"},"xaxis":{"title":{"text":"FLOPS"}},"yaxis":{"title":{"text":"accuracy"}},"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermap":[{"type":"scattermap","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}}},                        {"responsive": true}                    )                };            </script> </div>
</div>
<br/>
<br/><p>Fetch the list of trials on the Pareto front with <a class="reference internal" href="../../reference/generated/optuna.study.Study.html#optuna.study.Study.best_trials" title="optuna.study.Study.best_trials"><code class="xref py py-attr docutils literal notranslate"><span class="pre">best_trials</span></code></a>.</p>
<p>For example, the following code shows the number of trials on the Pareto front and picks the trial with the highest accuracy.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Number of trials on the Pareto front: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">study</span><span class="o">.</span><span class="n">best_trials</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">trial_with_highest_accuracy</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">study</span><span class="o">.</span><span class="n">best_trials</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Trial with highest accuracy: "</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\t</span><span class="s2">number: </span><span class="si">{</span><span class="n">trial_with_highest_accuracy</span><span class="o">.</span><span class="n">number</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\t</span><span class="s2">params: </span><span class="si">{</span><span class="n">trial_with_highest_accuracy</span><span class="o">.</span><span class="n">params</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\t</span><span class="s2">values: </span><span class="si">{</span><span class="n">trial_with_highest_accuracy</span><span class="o">.</span><span class="n">values</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Number of trials on the Pareto front: 5
Trial with highest accuracy:
        number: 20
        params: {'n_layers': 1, 'n_units_l0': 64, 'dropout_0': 0.4427151058715231, 'lr': 0.003991493208779521}
        values: [50816.0, 0.83203125]
</pre></div>
</div>
<p>Learn which hyperparameters are affecting the flops most with hyperparameter importance.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">optuna</span><span class="o">.</span><span class="n">visualization</span><span class="o">.</span><span class="n">plot_param_importances</span><span class="p">(</span>
    <span class="n">study</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">target_name</span><span class="o">=</span><span class="s2">"flops"</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="output_subarea output_html rendered_html output_result">
<div> <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
<script charset="utf-8" src="https://cdn.plot.ly/plotly-3.0.1.min.js"></script> <div class="plotly-graph-div" id="4a36dc7f-015a-45e5-8201-267604217aec" style="height:100%; width:100%;"></div> <script type="text/javascript">                window.PLOTLYENV=window.PLOTLYENV || {};                                if (document.getElementById("4a36dc7f-015a-45e5-8201-267604217aec")) {                    Plotly.newPlot(                        "4a36dc7f-015a-45e5-8201-267604217aec",                        [{"cliponaxis":false,"hovertemplate":["lr (FloatDistribution): 0.00029028919873346057\u003cextra\u003e\u003c\u002fextra\u003e","n_layers (IntDistribution): 0.001387871252898714\u003cextra\u003e\u003c\u002fextra\u003e","dropout_0 (FloatDistribution): 0.003070988901425808\u003cextra\u003e\u003c\u002fextra\u003e","n_units_l0 (IntDistribution): 0.9952508506469421\u003cextra\u003e\u003c\u002fextra\u003e"],"name":"flops","orientation":"h","text":["\u003c0.01","\u003c0.01","\u003c0.01","1.00"],"textposition":"outside","x":[0.00029028919873346057,0.001387871252898714,0.003070988901425808,0.9952508506469421],"y":["lr","n_layers","dropout_0","n_units_l0"],"type":"bar"}],                        {"title":{"text":"Hyperparameter Importances"},"xaxis":{"title":{"text":"Hyperparameter Importance"}},"yaxis":{"title":{"text":"Hyperparameter"}},"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermap":[{"type":"scattermap","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}}},                        {"responsive": true}                    )                };            </script> </div>
</div>
<br/>
<br/><p class="sphx-glr-timing"><strong>Total running time of the script:</strong> (1 minutes 55.765 seconds)</p>
<a class="dashAnchor" name="//apple_ref/cpp/Section/sphx_glr_download_tutorial_20_recipes_002_multi_objective.py"></a><div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-tutorial-20-recipes-002-multi-objective-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/a4ea522762978cb29312e091bc523c7a/002_multi_objective.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">002_multi_objective.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/82aa5c7e2a8d8749f9f6f65a6172eb9f/002_multi_objective.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">002_multi_objective.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-zip docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/4bf0db05e53701be6edd2d2db48ab6db/002_multi_objective.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">zipped:</span> <span class="pre">002_multi_objective.zip</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</section>
</section>
</div>
</div>
<footer>
<hr/>
<div role="contentinfo">
<p>© Copyright 2018, Optuna Contributors.</p>
</div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
  
    <a href="../../privacy.html">Privacy Policy</a>.
     


</footer>
</div>
</div>
</section>
</div>
<script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
</body>
</html>